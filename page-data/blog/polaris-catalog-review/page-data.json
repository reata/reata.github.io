{"componentChunkName":"component---src-pages-markdown-remark-frontmatter-slug-js","path":"/blog/polaris-catalog-review/","result":{"data":{"markdownRemark":{"html":"<p>在过去的十多年中，Hive Metastore几乎就是data catalog这个领域的事实标准。Hive的后来者无论是Presto还是Spark，要操作Hadoop上的表数据，都绕不开\nHive Metastore。而随着开放数据表格式的兴起，Iceberg、Hudi、Delta则都有自己的一套Catalog，完成针对具体table的建表、删表、重命名等任务，\n也包括维护所有托管的表，记录每张表最新的元数据。Delta有自己的Unity Catalog，Iceberg则在支持Hive Metastore作为catalog之外，\n也定义了相关的OpenAPI接口，支持REST类型的catalog。Snowflake开源并捐献给Apache基金会的Polaris Catalog，就是Iceberg REST catalog的一种实现。</p>\n<h2>Iceberg REST Catalog定义了哪些内容？</h2>\n<p>参考Iceberg的<a href=\"https://github.com/apache/iceberg/blob/main/open-api/rest-catalog-open-api.yaml\">OpenAPI spec</a>以及\n<a href=\"https://editor-next.swagger.io/?url=https://raw.githubusercontent.com/apache/polaris/refs/tags/apache-polaris-0.9.0-incubating/spec/rest-catalog-open-api.yaml\">Swagger UI</a>,\n这里提供的API包括namespace(也就是database)的增删改查，namespace下table和view的增删改查，以及Iceberg的事务操作，比如单一table的事务更新\n（即commit snapshot），以及针对多个table创建的事务。</p>\n<p>Iceberg的Catalog除去事务是特殊的之外，其它namespace/table/view的操作，都可以简单对应到Hive Metastore的API。</p>\n<h2>Polaris Catalog在Iceberg REST Catalog之上提供了什么额外的内容？</h2>\n<h3>多Catalog</h3>\n<p>Iceberg的REST Catalog是针对单一的catalog的specification，但查询引擎可以不限于只使用单个catalog。以Spark为例，配置时需要指定：</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">spark-sql <span class=\"token parameter variable\">--packages</span> org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0<span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--conf</span> <span class=\"token assign-left variable\">spark.sql.extensions</span><span class=\"token operator\">=</span>org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--conf</span> <span class=\"token assign-left variable\">spark.sql.catalog.spark_catalog</span><span class=\"token operator\">=</span>org.apache.iceberg.spark.SparkSessionCatalog <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--conf</span> <span class=\"token assign-left variable\">spark.sql.catalog.spark_catalog.type</span><span class=\"token operator\">=</span>hive <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--conf</span> <span class=\"token assign-left variable\">spark.sql.catalog.rest_prod</span><span class=\"token operator\">=</span>org.apache.iceberg.spark.SparkCatalog <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--conf</span> <span class=\"token assign-left variable\">spark.sql.catalog.rest_prod.type</span><span class=\"token operator\">=</span>rest <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--conf</span> <span class=\"token assign-left variable\">spark.sql.catalog.rest_prod.uri</span><span class=\"token operator\">=</span>http://localhost:8080 <span class=\"token punctuation\">\\</span>\n    <span class=\"token parameter variable\">--conf</span> <span class=\"token assign-left variable\">spark.sql.defaultCatalog</span><span class=\"token operator\">=</span>local</code></pre></div>\n<p>这样在同一个Spark session中，Hive Metastore和Iceberg相关的REST Catalog中的表，都可以一起使用。类似的，Spark也可以指定多个Iceberg REST catalog，\n而Polaris是支持创建多个Catalog的。</p>\n<h3>权限控制</h3>\n<p>Polaris支持向不同的查询引擎实例提供不同的权限认证。比如总共创建了三个Catalog: Catalog1, Catalog2, Catalog3。而公司内部有多套不同的查询引擎：\n市场部门的Snowflake1，财务部门的Snowflake2，数据部门的Spark。可以配置出Spark允许访问Catalog 1,2,3；Snowflake1只允许访问Catalog 1；\nSnowflake2只允许访问Catalog 2这样的权限认证模式。</p>\n<p>另外每个Catalog底下可以对应不同的存储介质，比如S3, GCS, Azure等。Polaris也托管了到这些对象存储的认证信息，并在查询执行时，\n将相应的认证信息提供给查询引擎。这样Spark/Snowflake等可以不需要留存对S3, GCS, Azure的访问密钥。</p>\n<h2>Polaris如何实现REST Catalog</h2>\n<p><strong><code>polaris-api-iceberg-service</code></strong>: 包含了通过openapi.yaml进行codegen之后的controller部分的代码，以及service的interface。</p>\n<p><strong><code>polaris-service-common</code></strong>: 以Adapter的形式，具体实现了上面的interface。</p>\n<p><strong><code>polaris-relational-jdbc</code></strong>: 数据库持久层，Polaris里将其称作Metastores，将catalog, namespace, table等元数据持久化到PostgreSQL.</p>\n<p>另外核心的业务逻辑在<code>polaris-core</code>，而Web Service是通过<strong>quarkus</strong>来实现的，所以有<code>polaris-quarkus-*</code>的一系列模块。</p>\n<h2>结语</h2>\n<p>Polaris能否在长期持续演化，并在Iceberg生态链中扮演曾经Hive Metastore之于Hadoop的作用，让我们拭目以待。</p>","rawMarkdownBody":"\n在过去的十多年中，Hive Metastore几乎就是data catalog这个领域的事实标准。Hive的后来者无论是Presto还是Spark，要操作Hadoop上的表数据，都绕不开\nHive Metastore。而随着开放数据表格式的兴起，Iceberg、Hudi、Delta则都有自己的一套Catalog，完成针对具体table的建表、删表、重命名等任务，\n也包括维护所有托管的表，记录每张表最新的元数据。Delta有自己的Unity Catalog，Iceberg则在支持Hive Metastore作为catalog之外，\n也定义了相关的OpenAPI接口，支持REST类型的catalog。Snowflake开源并捐献给Apache基金会的Polaris Catalog，就是Iceberg REST catalog的一种实现。\n\n## Iceberg REST Catalog定义了哪些内容？\n参考Iceberg的[OpenAPI spec](https://github.com/apache/iceberg/blob/main/open-api/rest-catalog-open-api.yaml)以及\n[Swagger UI](https://editor-next.swagger.io/?url=https://raw.githubusercontent.com/apache/polaris/refs/tags/apache-polaris-0.9.0-incubating/spec/rest-catalog-open-api.yaml),\n这里提供的API包括namespace(也就是database)的增删改查，namespace下table和view的增删改查，以及Iceberg的事务操作，比如单一table的事务更新\n（即commit snapshot），以及针对多个table创建的事务。\n\nIceberg的Catalog除去事务是特殊的之外，其它namespace/table/view的操作，都可以简单对应到Hive Metastore的API。\n\n## Polaris Catalog在Iceberg REST Catalog之上提供了什么额外的内容？\n\n### 多Catalog\nIceberg的REST Catalog是针对单一的catalog的specification，但查询引擎可以不限于只使用单个catalog。以Spark为例，配置时需要指定：\n\n```shell\nspark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.9.0\\\n    --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n    --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \\\n    --conf spark.sql.catalog.spark_catalog.type=hive \\\n    --conf spark.sql.catalog.rest_prod=org.apache.iceberg.spark.SparkCatalog \\\n    --conf spark.sql.catalog.rest_prod.type=rest \\\n    --conf spark.sql.catalog.rest_prod.uri=http://localhost:8080 \\\n    --conf spark.sql.defaultCatalog=local\n```\n\n这样在同一个Spark session中，Hive Metastore和Iceberg相关的REST Catalog中的表，都可以一起使用。类似的，Spark也可以指定多个Iceberg REST catalog，\n而Polaris是支持创建多个Catalog的。\n\n### 权限控制\n\nPolaris支持向不同的查询引擎实例提供不同的权限认证。比如总共创建了三个Catalog: Catalog1, Catalog2, Catalog3。而公司内部有多套不同的查询引擎：\n市场部门的Snowflake1，财务部门的Snowflake2，数据部门的Spark。可以配置出Spark允许访问Catalog 1,2,3；Snowflake1只允许访问Catalog 1；\nSnowflake2只允许访问Catalog 2这样的权限认证模式。\n\n另外每个Catalog底下可以对应不同的存储介质，比如S3, GCS, Azure等。Polaris也托管了到这些对象存储的认证信息，并在查询执行时，\n将相应的认证信息提供给查询引擎。这样Spark/Snowflake等可以不需要留存对S3, GCS, Azure的访问密钥。\n\n## Polaris如何实现REST Catalog\n\n**`polaris-api-iceberg-service`**: 包含了通过openapi.yaml进行codegen之后的controller部分的代码，以及service的interface。\n\n**`polaris-service-common`**: 以Adapter的形式，具体实现了上面的interface。\n\n**`polaris-relational-jdbc`**: 数据库持久层，Polaris里将其称作Metastores，将catalog, namespace, table等元数据持久化到PostgreSQL.\n\n另外核心的业务逻辑在`polaris-core`，而Web Service是通过**quarkus**来实现的，所以有`polaris-quarkus-*`的一系列模块。\n\n## 结语\n\nPolaris能否在长期持续演化，并在Iceberg生态链中扮演曾经Hive Metastore之于Hadoop的作用，让我们拭目以待。\n","frontmatter":{"date":"May 11, 2025","slug":"/blog/polaris-catalog-review","title":"Polaris Catalog浅探","excerpt":"Iceberg时代的Hive Metastore","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#081818","images":{"fallback":{"src":"/static/7ea3899b269315404b74761e76016fbc/7659a/polaris.png","srcSet":"/static/7ea3899b269315404b74761e76016fbc/0abbd/polaris.png 840w,\n/static/7ea3899b269315404b74761e76016fbc/50d48/polaris.png 1680w,\n/static/7ea3899b269315404b74761e76016fbc/7659a/polaris.png 3360w","sizes":"(min-width: 3360px) 3360px, 100vw"},"sources":[{"srcSet":"/static/7ea3899b269315404b74761e76016fbc/a0d53/polaris.webp 840w,\n/static/7ea3899b269315404b74761e76016fbc/2f6d3/polaris.webp 1680w,\n/static/7ea3899b269315404b74761e76016fbc/2d2bf/polaris.webp 3360w","type":"image/webp","sizes":"(min-width: 3360px) 3360px, 100vw"}]},"width":3360,"height":1440}}}}}},"pageContext":{"id":"7faeb5dd-5973-570a-922e-548eda13ac4b","frontmatter__slug":"/blog/polaris-catalog-review","__params":{"frontmatter__slug":"blog"}}},"staticQueryHashes":[]}