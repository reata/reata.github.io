{"componentChunkName":"component---src-pages-markdown-remark-frontmatter-slug-js","path":"/blog/smallpond-review/","result":{"data":{"markdownRemark":{"html":"<p><a href=\"https://github.com/deepseek-ai/smallpond\">Smallpond</a> 是DeepSeek开源的一个基于DuckDB的轻量级数据处理框架。在Andy Pavlo教授的年度回顾中\n<a href=\"https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html\">Databases in 2024: A Year in Review</a>，\n他就提到在OLAP领域中，其实绝大多数的query处理的数据量都很小，中位数在100MB(而目前AWS上EC2提供的最大内存的实例为24TB），这意味着使用单实例的\nDuckDB已经足够。更不必说使用DuckDB同时意味着原生就有了列式存储和向量执行的支持，性能优越。如今很多数据库都在开始集成DuckDB作为查询引擎。</p>\n<p>Smallpond将DuckDB引入了数据处理领域，和Spark来到了同一战场。这里面就有很多值得一探的地方了。</p>\n<h2>Smallpond的编程API</h2>\n<p>Smallpond有一套自己的DataFrame定义，针对DataFrame有map, filter, flat_map, repartition等等操作，所有操作都是惰性执行的，和Spark高度类似。\n每个DataFrame都包含一个logical plan，map/filter这些操作会生成一个新的DataFrame，持有一个变更过的logical plan。</p>\n<p>每一个plan都是一个Node，同时Node有自己的input_deps，可以追溯到直接依赖的上游Node。Node为点，input_deps为边，就共同构成一个DAG。\n举例来说，像<code>df2=df1.map(col=1)</code>这样一段代码，df2的plan就是SqlEngineNode，并且这个SqlEngineNode通过input_deps参数，指向df1的Node。\ndf1可以是一个单独的读取文件系统的Node，也可以是已经经过多次操作嵌套的plan。</p>\n<p>Node具体的类型，和Spark大差不差。有类似DataSourceNode, ShuffleNode, HashPartitionNode, SqlEngineNode, DataSinkNode等等。\n所有的Node都持有同一session的Context，Context里面有node_id的计数，会自动自增。</p>\n<p>注意到像map, filter这些操作，在Spark中就是一个Project或者Filter算子，但是在Smallpond中，都是SqlEngineNode。SqlEngineNode并行地在每个\npartition上通过调用DuckDB来执行。map对应的sql就是<code>select {expr} from {df}</code>, filter对应<code>select * from {df} where ({expr})</code></p>\n<h2>Smallpond支持SQL API吗？</h2>\n<p>Spark的SQL API和DataFrame API，都可以解析成相同的Logical Plan。就SQL而言，一个复杂的SQL是会拆成多个任务来执行，中间可能涉及到shuffle。\n这也是从MapReduce, Hive一路继承而来的。</p>\n<p>而Smallpond是不会自动完成任务的拆分的。比如Readme里<a href=\"https://github.com/deepseek-ai/smallpond/tree/v0.15.0?tab=readme-ov-file#quick-start\">Quick Start</a>的例子：</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> smallpond\n\n<span class=\"token comment\"># Initialize session</span>\nsp <span class=\"token operator\">=</span> smallpond<span class=\"token punctuation\">.</span>init<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Load data</span>\ndf <span class=\"token operator\">=</span> sp<span class=\"token punctuation\">.</span>read_parquet<span class=\"token punctuation\">(</span><span class=\"token string\">\"prices.parquet\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Process data</span>\ndf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> hash_by<span class=\"token operator\">=</span><span class=\"token string\">\"ticker\"</span><span class=\"token punctuation\">)</span>\ndf <span class=\"token operator\">=</span> sp<span class=\"token punctuation\">.</span>partial_sql<span class=\"token punctuation\">(</span><span class=\"token string\">\"SELECT ticker, min(price), max(price) FROM {0} GROUP BY ticker\"</span><span class=\"token punctuation\">,</span> df<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Save results</span>\ndf<span class=\"token punctuation\">.</span>write_parquet<span class=\"token punctuation\">(</span><span class=\"token string\">\"output/\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Show results</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">.</span>to_pandas<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>要执行的sql是：</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> ticker<span class=\"token punctuation\">,</span> <span class=\"token function\">min</span><span class=\"token punctuation\">(</span>price<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token function\">max</span><span class=\"token punctuation\">(</span>price<span class=\"token punctuation\">)</span> <span class=\"token keyword\">FROM</span> {<span class=\"token number\">0</span>} <span class=\"token keyword\">GROUP</span> <span class=\"token keyword\">BY</span> ticker</code></pre></div>\n<p>就Spark而言，我们知道这里会用ticker作为key，对整个数据集进行shuffle，按<code>spark.sql.shuffle.partitions</code>参数进行自动分为若干个分区，\n再并行地对每个分区上的结果执行计算。</p>\n<p>而Smallpond需要用户显式地根据ticker来做repartition。这也是为啥它的操作叫做<strong>partial_sql</strong>. 如果这里用户指定的hash_by字段不对，\n或者使用的是其它的分区策略，那么出来的计算结果就是错的。</p>\n<p><strong>partial_sql</strong>的函数注释里也明确了，如果要对多个df做join，用户需要自己保证两个df的是按照join key具有相同的分区的。</p>\n<p>这也是Smallpond目前和Spark最大功能差距。暂时Smallpond还是一个轻量级数据处理框架，并不能完全覆盖整个数据处理的场景。\n编写代码的用户需要对底层分布式执行的细节有更大程度的理解。</p>\n<h2>Smallpond的Optimizer做了什么</h2>\n<p>前面提到，DataFrame是惰性计算的，针对DataFrame的每个操作，都在现有plan的基础上，继续添加节点。</p>\n<p>而最终DataFrame在执行之前，会走Optimizer<a href=\"https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/dataframe.py#L235\">生成optimized_plan</a>。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">self<span class=\"token punctuation\">.</span>optimized_plan <span class=\"token operator\">=</span> Optimizer<span class=\"token punctuation\">(</span>exclude_nodes<span class=\"token operator\">=</span><span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>session<span class=\"token punctuation\">.</span>_node_to_tasks<span class=\"token punctuation\">.</span>keys<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>visit<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>plan<span class=\"token punctuation\">)</span></code></pre></div>\n<p><a href=\"https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/logical/optimizer.py\">Optimizer</a>目前只实现了\nvisit_query_engine_node，也就是针对DuckDB的算子。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">visit_query_engine_node</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> node<span class=\"token punctuation\">:</span> SqlEngineNode<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">:</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> Node<span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># fuse consecutive SqlEngineNodes</span>\n    <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">.</span>input_deps<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">1</span> <span class=\"token keyword\">and</span> <span class=\"token builtin\">isinstance</span><span class=\"token punctuation\">(</span>\n        child <span class=\"token operator\">:=</span> self<span class=\"token punctuation\">.</span>visit<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">.</span>input_deps<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> depth <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> SqlEngineNode\n    <span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        fused <span class=\"token operator\">=</span> copy<span class=\"token punctuation\">.</span>copy<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">)</span>\n        fused<span class=\"token punctuation\">.</span>input_deps <span class=\"token operator\">=</span> child<span class=\"token punctuation\">.</span>input_deps\n        fused<span class=\"token punctuation\">.</span>udfs <span class=\"token operator\">=</span> node<span class=\"token punctuation\">.</span>udfs <span class=\"token operator\">+</span> child<span class=\"token punctuation\">.</span>udfs\n        fused<span class=\"token punctuation\">.</span>cpu_limit <span class=\"token operator\">=</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">.</span>cpu_limit<span class=\"token punctuation\">,</span> child<span class=\"token punctuation\">.</span>cpu_limit<span class=\"token punctuation\">)</span>\n        fused<span class=\"token punctuation\">.</span>gpu_limit <span class=\"token operator\">=</span> <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">.</span>gpu_limit<span class=\"token punctuation\">,</span> child<span class=\"token punctuation\">.</span>gpu_limit<span class=\"token punctuation\">)</span>\n        fused<span class=\"token punctuation\">.</span>memory_limit <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>\n            <span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">.</span>memory_limit<span class=\"token punctuation\">,</span> child<span class=\"token punctuation\">.</span>memory_limit<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">if</span> node<span class=\"token punctuation\">.</span>memory_limit <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span> <span class=\"token keyword\">and</span> child<span class=\"token punctuation\">.</span>memory_limit <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span>\n            <span class=\"token keyword\">else</span> node<span class=\"token punctuation\">.</span>memory_limit <span class=\"token keyword\">or</span> child<span class=\"token punctuation\">.</span>memory_limit\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># merge the sql queries</span>\n        <span class=\"token comment\"># example:</span>\n        <span class=\"token comment\"># ```</span>\n        <span class=\"token comment\"># child.sql_queries = [\"select * from {0}\"]</span>\n        <span class=\"token comment\">#  node.sql_queries = [\"select a, b from {0}\"]</span>\n        <span class=\"token comment\"># fused.sql_queries = [\"select a, b from (select * from {0})\"]</span>\n        <span class=\"token comment\"># ```</span>\n        fused<span class=\"token punctuation\">.</span>sql_queries <span class=\"token operator\">=</span> child<span class=\"token punctuation\">.</span>sql_queries<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span> <span class=\"token punctuation\">[</span>\n            query<span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"(</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>child<span class=\"token punctuation\">.</span>sql_queries<span class=\"token punctuation\">[</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">)\"</span></span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> query <span class=\"token keyword\">in</span> node<span class=\"token punctuation\">.</span>sql_queries\n        <span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">return</span> fused\n    <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>generic_visit<span class=\"token punctuation\">(</span>node<span class=\"token punctuation\">,</span> depth<span class=\"token punctuation\">)</span></code></pre></div>\n<p>这里具体的优化逻辑是，如果连续两个算子都是sql算子，那么通过子查询的形式，将sql合并，两个算子变成一个算子，少调用一次DuckDB，算是一个非常直接易懂的优化规则。</p>\n<h2>Planner: Smallpond和Ray的集成</h2>\n<p>生成optimized plan之后，planner进一步<a href=\"https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/dataframe.py#L259\">生成执行计划</a>。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># create tasks for the optimized plan</span>\nplanner <span class=\"token operator\">=</span> Planner<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>session<span class=\"token punctuation\">.</span>_runtime_ctx<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># let planner update self.session._node_to_tasks</span>\nplanner<span class=\"token punctuation\">.</span>node_to_tasks <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>session<span class=\"token punctuation\">.</span>_node_to_tasks\n<span class=\"token keyword\">return</span> planner<span class=\"token punctuation\">.</span>visit<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>optimized_plan<span class=\"token punctuation\">)</span></code></pre></div>\n<p>具体来说，Planner把Optimized Plan里的Node，转换为Task。Task基本就是针对Ray Core里的Low Level API: Ray Task的抽象。</p>\n<p><a href=\"https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/dataframe.py#L285\">task.run_on_ray()</a>，\n会创建<a href=\"https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/execution/task.py#L1127\">在ray上的执行任务</a>，\ntask在ray上执行完成后，会返回一个DataSet。</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@ray<span class=\"token punctuation\">.</span>remote</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">exec_task</span><span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">:</span> Task<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>inputs<span class=\"token punctuation\">:</span> DataSet<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> DataSet<span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">import</span> multiprocessing <span class=\"token keyword\">as</span> mp\n    <span class=\"token keyword\">import</span> os\n    <span class=\"token keyword\">from</span> pathlib <span class=\"token keyword\">import</span> Path\n\n    <span class=\"token keyword\">from</span> loguru <span class=\"token keyword\">import</span> logger\n\n    <span class=\"token comment\"># ray use a process pool to execute tasks</span>\n    <span class=\"token comment\"># we set the current process name to the task name</span>\n    <span class=\"token comment\"># so that we can see task name in the logs</span>\n    mp<span class=\"token punctuation\">.</span>current_process<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>name <span class=\"token operator\">=</span> task<span class=\"token punctuation\">.</span>key\n\n    <span class=\"token comment\"># probe the retry count</span>\n    task<span class=\"token punctuation\">.</span>retry_count <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">while</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">.</span>ray_marker_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        task<span class=\"token punctuation\">.</span>retry_count <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n        <span class=\"token keyword\">if</span> task<span class=\"token punctuation\">.</span>retry_count <span class=\"token operator\">></span> DEFAULT_MAX_RETRY_COUNT<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">raise</span> RuntimeError<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"task </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>task<span class=\"token punctuation\">.</span>key<span class=\"token punctuation\">}</span></span><span class=\"token string\"> failed after </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>task<span class=\"token punctuation\">.</span>retry_count<span class=\"token punctuation\">}</span></span><span class=\"token string\"> retries\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> task<span class=\"token punctuation\">.</span>retry_count <span class=\"token operator\">></span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span>\n        logger<span class=\"token punctuation\">.</span>warning<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"task </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>task<span class=\"token punctuation\">.</span>key<span class=\"token punctuation\">}</span></span><span class=\"token string\"> is being retried for the </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>task<span class=\"token punctuation\">.</span>retry_count<span class=\"token punctuation\">}</span></span><span class=\"token string\">th time\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># create the marker file</span>\n    Path<span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">.</span>ray_marker_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>touch<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># put the inputs into the task</span>\n    <span class=\"token keyword\">assert</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>inputs<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">.</span>input_deps<span class=\"token punctuation\">)</span>\n    task<span class=\"token punctuation\">.</span>input_datasets <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>inputs<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># execute the task</span>\n    status <span class=\"token operator\">=</span> task<span class=\"token punctuation\">.</span><span class=\"token keyword\">exec</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> status <span class=\"token operator\">!=</span> WorkStatus<span class=\"token punctuation\">.</span>SUCCEED<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">raise</span> task<span class=\"token punctuation\">.</span>exception <span class=\"token keyword\">or</span> RuntimeError<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"task </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>task<span class=\"token punctuation\">.</span>key<span class=\"token punctuation\">}</span></span><span class=\"token string\"> failed with status </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>status<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># dump the output dataset atomically</span>\n    os<span class=\"token punctuation\">.</span>makedirs<span class=\"token punctuation\">(</span>os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>dirname<span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">.</span>ray_dataset_path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> exist_ok<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    dump<span class=\"token punctuation\">(</span>task<span class=\"token punctuation\">.</span>output<span class=\"token punctuation\">,</span> task<span class=\"token punctuation\">.</span>ray_dataset_path<span class=\"token punctuation\">,</span> atomic_write<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> task<span class=\"token punctuation\">.</span>output</code></pre></div>\n<p>DataFrame API -> Logical Plan -> Optimized Plan -> Execution Task，算是一个非常典型的数据处理框架的架构。而针对DuckDB的使用，\n是非常具有启发性的。</p>","rawMarkdownBody":"\n[Smallpond](https://github.com/deepseek-ai/smallpond) 是DeepSeek开源的一个基于DuckDB的轻量级数据处理框架。在Andy Pavlo教授的年度回顾中\n[Databases in 2024: A Year in Review](https://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html)，\n他就提到在OLAP领域中，其实绝大多数的query处理的数据量都很小，中位数在100MB(而目前AWS上EC2提供的最大内存的实例为24TB），这意味着使用单实例的\nDuckDB已经足够。更不必说使用DuckDB同时意味着原生就有了列式存储和向量执行的支持，性能优越。如今很多数据库都在开始集成DuckDB作为查询引擎。\n\nSmallpond将DuckDB引入了数据处理领域，和Spark来到了同一战场。这里面就有很多值得一探的地方了。\n\n## Smallpond的编程API\nSmallpond有一套自己的DataFrame定义，针对DataFrame有map, filter, flat_map, repartition等等操作，所有操作都是惰性执行的，和Spark高度类似。\n每个DataFrame都包含一个logical plan，map/filter这些操作会生成一个新的DataFrame，持有一个变更过的logical plan。\n\n每一个plan都是一个Node，同时Node有自己的input_deps，可以追溯到直接依赖的上游Node。Node为点，input_deps为边，就共同构成一个DAG。\n举例来说，像`df2=df1.map(col=1)`这样一段代码，df2的plan就是SqlEngineNode，并且这个SqlEngineNode通过input_deps参数，指向df1的Node。\ndf1可以是一个单独的读取文件系统的Node，也可以是已经经过多次操作嵌套的plan。\n\nNode具体的类型，和Spark大差不差。有类似DataSourceNode, ShuffleNode, HashPartitionNode, SqlEngineNode, DataSinkNode等等。\n所有的Node都持有同一session的Context，Context里面有node_id的计数，会自动自增。\n\n注意到像map, filter这些操作，在Spark中就是一个Project或者Filter算子，但是在Smallpond中，都是SqlEngineNode。SqlEngineNode并行地在每个\npartition上通过调用DuckDB来执行。map对应的sql就是`select {expr} from {df}`, filter对应`select * from {df} where ({expr})`\n\n## Smallpond支持SQL API吗？\n\nSpark的SQL API和DataFrame API，都可以解析成相同的Logical Plan。就SQL而言，一个复杂的SQL是会拆成多个任务来执行，中间可能涉及到shuffle。\n这也是从MapReduce, Hive一路继承而来的。\n\n而Smallpond是不会自动完成任务的拆分的。比如Readme里[Quick Start](https://github.com/deepseek-ai/smallpond/tree/v0.15.0?tab=readme-ov-file#quick-start)的例子：\n\n```python\nimport smallpond\n\n# Initialize session\nsp = smallpond.init()\n\n# Load data\ndf = sp.read_parquet(\"prices.parquet\")\n\n# Process data\ndf = df.repartition(3, hash_by=\"ticker\")\ndf = sp.partial_sql(\"SELECT ticker, min(price), max(price) FROM {0} GROUP BY ticker\", df)\n\n# Save results\ndf.write_parquet(\"output/\")\n# Show results\nprint(df.to_pandas())\n```\n\n要执行的sql是：\n\n```sql\nSELECT ticker, min(price), max(price) FROM {0} GROUP BY ticker\n```\n\n就Spark而言，我们知道这里会用ticker作为key，对整个数据集进行shuffle，按`spark.sql.shuffle.partitions`参数进行自动分为若干个分区，\n再并行地对每个分区上的结果执行计算。\n\n而Smallpond需要用户显式地根据ticker来做repartition。这也是为啥它的操作叫做**partial_sql**. 如果这里用户指定的hash_by字段不对，\n或者使用的是其它的分区策略，那么出来的计算结果就是错的。\n\n**partial_sql**的函数注释里也明确了，如果要对多个df做join，用户需要自己保证两个df的是按照join key具有相同的分区的。\n\n这也是Smallpond目前和Spark最大功能差距。暂时Smallpond还是一个轻量级数据处理框架，并不能完全覆盖整个数据处理的场景。\n编写代码的用户需要对底层分布式执行的细节有更大程度的理解。\n\n## Smallpond的Optimizer做了什么\n\n前面提到，DataFrame是惰性计算的，针对DataFrame的每个操作，都在现有plan的基础上，继续添加节点。\n\n而最终DataFrame在执行之前，会走Optimizer[生成optimized_plan](https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/dataframe.py#L235)。\n\n```python\nself.optimized_plan = Optimizer(exclude_nodes=set(self.session._node_to_tasks.keys())).visit(self.plan)\n```\n\n[Optimizer](https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/logical/optimizer.py)目前只实现了\nvisit_query_engine_node，也就是针对DuckDB的算子。\n\n```python\ndef visit_query_engine_node(self, node: SqlEngineNode, depth: int) -> Node:\n    # fuse consecutive SqlEngineNodes\n    if len(node.input_deps) == 1 and isinstance(\n        child := self.visit(node.input_deps[0], depth + 1), SqlEngineNode\n    ):\n        fused = copy.copy(node)\n        fused.input_deps = child.input_deps\n        fused.udfs = node.udfs + child.udfs\n        fused.cpu_limit = max(node.cpu_limit, child.cpu_limit)\n        fused.gpu_limit = max(node.gpu_limit, child.gpu_limit)\n        fused.memory_limit = (\n            max(node.memory_limit, child.memory_limit)\n            if node.memory_limit is not None and child.memory_limit is not None\n            else node.memory_limit or child.memory_limit\n        )\n        # merge the sql queries\n        # example:\n        # ```\n        # child.sql_queries = [\"select * from {0}\"]\n        #  node.sql_queries = [\"select a, b from {0}\"]\n        # fused.sql_queries = [\"select a, b from (select * from {0})\"]\n        # ```\n        fused.sql_queries = child.sql_queries[:-1] + [\n            query.format(f\"({child.sql_queries[-1]})\") for query in node.sql_queries\n        ]\n        return fused\n    return self.generic_visit(node, depth)\n```\n\n这里具体的优化逻辑是，如果连续两个算子都是sql算子，那么通过子查询的形式，将sql合并，两个算子变成一个算子，少调用一次DuckDB，算是一个非常直接易懂的优化规则。\n\n## Planner: Smallpond和Ray的集成\n\n生成optimized plan之后，planner进一步[生成执行计划](https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/dataframe.py#L259)。\n\n```python\n# create tasks for the optimized plan\nplanner = Planner(self.session._runtime_ctx)\n# let planner update self.session._node_to_tasks\nplanner.node_to_tasks = self.session._node_to_tasks\nreturn planner.visit(self.optimized_plan)\n```\n\n具体来说，Planner把Optimized Plan里的Node，转换为Task。Task基本就是针对Ray Core里的Low Level API: Ray Task的抽象。\n\n[task.run_on_ray()](https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/dataframe.py#L285)，\n会创建[在ray上的执行任务](https://github.com/deepseek-ai/smallpond/blob/v0.15.0/smallpond/execution/task.py#L1127)，\ntask在ray上执行完成后，会返回一个DataSet。\n\n```python\n@ray.remote\ndef exec_task(task: Task, *inputs: DataSet) -> DataSet:\n    import multiprocessing as mp\n    import os\n    from pathlib import Path\n\n    from loguru import logger\n\n    # ray use a process pool to execute tasks\n    # we set the current process name to the task name\n    # so that we can see task name in the logs\n    mp.current_process().name = task.key\n\n    # probe the retry count\n    task.retry_count = 0\n    while os.path.exists(task.ray_marker_path):\n        task.retry_count += 1\n        if task.retry_count > DEFAULT_MAX_RETRY_COUNT:\n            raise RuntimeError(f\"task {task.key} failed after {task.retry_count} retries\")\n    if task.retry_count > 0:\n        logger.warning(f\"task {task.key} is being retried for the {task.retry_count}th time\")\n    # create the marker file\n    Path(task.ray_marker_path).touch()\n\n    # put the inputs into the task\n    assert len(inputs) == len(task.input_deps)\n    task.input_datasets = list(inputs)\n    # execute the task\n    status = task.exec()\n    if status != WorkStatus.SUCCEED:\n        raise task.exception or RuntimeError(f\"task {task.key} failed with status {status}\")\n\n    # dump the output dataset atomically\n    os.makedirs(os.path.dirname(task.ray_dataset_path), exist_ok=True)\n    dump(task.output, task.ray_dataset_path, atomic_write=True)\n    return task.output\n```\n\nDataFrame API -> Logical Plan -> Optimized Plan -> Execution Task，算是一个非常典型的数据处理框架的架构。而针对DuckDB的使用，\n是非常具有启发性的。\n","frontmatter":{"date":"May 01, 2025","slug":"/blog/smallpond-review","title":"smallpond浅探","excerpt":"Deepseek AI开源的数据处理框架","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/4df245044384931a54189baa4595d4ef/a6312/smallpond.png","srcSet":"/static/4df245044384931a54189baa4595d4ef/e20c6/smallpond.png 300w,\n/static/4df245044384931a54189baa4595d4ef/62ea7/smallpond.png 600w,\n/static/4df245044384931a54189baa4595d4ef/a6312/smallpond.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/4df245044384931a54189baa4595d4ef/b979a/smallpond.webp 300w,\n/static/4df245044384931a54189baa4595d4ef/2fa07/smallpond.webp 600w,\n/static/4df245044384931a54189baa4595d4ef/f9756/smallpond.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":600}}}}}},"pageContext":{"id":"ee1bb326-9ab2-5be6-bd0d-6cb66e10c63e","frontmatter__slug":"/blog/smallpond-review","__params":{"frontmatter__slug":"blog"}}},"staticQueryHashes":[]}