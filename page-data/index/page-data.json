{"componentChunkName":"component---src-pages-index-js","path":"/","result":{"data":{"allMarkdownRemark":{"nodes":[{"rawMarkdownBody":"\n实时数据大概可以算是数据开发人员所面临的最常见的伪需求。实时的Dashboard老实说并不会带来如想象中那么多的数据洞见，但其带来的心理感受足以安抚一大片非技术人员。看着当天的实时数字不断跳动，公司的投入毕竟没有白费。\n\n于是工程师们一言不合地搭建起实时数据管道，一番工作后，一切就绪，打开浏览器，竟愕然发现，浏览器在不断地刷新！所以说，实时数据都是刷新出来的！公司有没有新的订单，完完全全取决于浏览器它刷新不刷新？！不刷新，我们就没有新订单，这也太反直觉了吧！于是兼职前端的Web开发终于放弃了上古时代的技术，用Ajax做了数据异步加载，刷新的过程用户变得无法感知。但仍然，这不是一个真正意义上的实时系统。\n\n真正意义的实时系统，我们希望用户在下单这个动作发生时，更准确地说，通常是在支付成功时，后台的数据系统里可以及时看到今日订单数据的变化。所谓及时，就是说除去必不可少的系统损耗外，数据流向的整条链路上是不应该存在轮询动作的，整个流程都应该由支付这个事件驱动。不必说，轮询还意味着资源的浪费。\n\nHTTP1.1及之前的协议，都建立在请求-响应模型之上，所有的通信需要客户端（也就是浏览器）主动发起。服务端即便感知到支付事件，也没有机制可以通过HTTP协议将消息推送到浏览器。而**WebSocket**协议实现了服务端和浏览器的双向通信，本篇博文将以Websocket为起点，从前端逐步往后地探讨如何打造整个基于**事件驱动**的实时数据系统。代码示例使用`Tornado`框架，可以在这个[仓库](https://github.com/reata/real-time-dashboard-example)中找到。\n\n![websocket](../images/websocket.png)\n\n## 洪荒时代，Monolithic架构\n\n作为一个minimum viable product，代码参见`feature/v0.1_monolithic`分支。简单设计了两个接口：\n\n订单模块的`ApiHandler`用来处理第三方支付平台的支付成功回调信息。在整个请求-响应过程中，通过DB操作，更新订单信息。HTTP请求结束之后，查询当前的订单信息，将该消息发送到报表模块所有的WebSocket客户端：\n\n``` python\nclass ApiHandler(web.RequestHandler):\n    \"\"\"订单模块支付宝支付成功回调接口\"\"\"\n\n    @web.asynchronous\n    def get(self, *args):\n        # 回调逻辑，数据库写入\n        order_id = int(self.get_argument(\"id\"))\n        order_amount = int(self.get_argument(\"amount\"))\n\n        new_order_flag = False\n        if session.query(Order.id).filter_by(id=order_id).scalar():\n            # 支付成功重复回调的情况，严谨起见还应该处理回调信息不一致的情况\n            print(\"重复回调\")\n            pass\n        else:\n            # 增加订单记录；现实世界中，这里通常是改变订单的状态\n            session.add(Order(id=order_id, amount=order_amount))\n            session.commit()\n            new_order_flag = True\n        self.finish()\n        # 请求完成后，异步通过ws发送消息到客户端\n        if new_order_flag:\n            data = {\"cnt\": session.query(Order).count(), \"amount\": session.query(func.sum(Order.amount)).scalar()}\n            data = json.dumps(data)\n            for c in rpt_ws_cl:\n                c.write_message(data)\n```\n\n报表模块的`SocketHandler`接受浏览器发出的WebSocket连接请求，将发起连接的用户加入到`rpt_ws_cl`变量中，并查询订单模块的数据库获取当前数据作为初始值，发送给客户端。\n\n```python                \nclass SocketHandler(websocket.WebSocketHandler):\n    \"\"\"报表模块websocket连接\"\"\"\n\n    def check_origin(self, origin):\n        return True\n\n    def open(self):\n        # 查询今日订单\n        if self not in rpt_ws_cl:\n            rpt_ws_cl.append(self)\n            self.write_message(json.dumps({\"cnt\": session.query(Order).count(),\n                                           \"amount\": session.query(func.sum(Order.amount)).scalar()}))\n\n    def on_close(self):\n        if self in rpt_ws_cl:\n            rpt_ws_cl.remove(self)\n```\n\n这个阶段的架构图如下所示：\n\n<img src=\"https://raw.githubusercontent.com/reata/real-time-dashboard-example/feature/v0.1_monolithic/architecture.png\" width=\"100%\" height=\"100%\">\n\n这里尽管我们区分了报表模块和订单模块，但实际上他们属于同一个Tornado应用。并且，模块之间的边界也很模糊。订单模块在收到支付回调请求，完成自身逻辑后，直接访问了报表模块的Websocket客户端列表。而报表模块在收到客户端连接请求时，为了获取初始数据，也直接访问了订单模块的数据库。它们本不应该这样通信。在这样的交互逻辑下，我们认为所谓报表模块只是订单模块所提供的一个管理后台也无不可。\n\n但事实不是这样，可以想见，当业务量扩大之后，报表模块会需要来自更多模块的数据，比如网站的PV/UV，这是订单模块所不能提供的。订单和报表从逻辑上说就不是一体。\n\n模块切分的另一个用意在于，当业务发展到某个阶段，流量爆发式增长，在已经做了读写分离方案的前提下，连写的数据库实例都达到了瓶颈，这时我们可以非常容易地对各个模块进行单独发布，打散单个模块的请求和数据库访问压力。\n\n服务拆分成最小不可分单元，还遇到写瓶颈，就要考虑对数据库进行水平分片，也就是Sharding的概念，俗称分库分表。恭喜你，业务量级来到这一步的话，你们的产品想必离成功不远了。但这是后话。暂时不必想这么远，时刻谨记，**过早的优化是万恶之源**。我们暂且只考虑垂直拆分。\n\n## 拆，必须要拆！\n\n决定要拆分模块后，首先要考虑的问题是，拆分后的模块之间如何通信？一个显而易见的答案是，通过API通信。HTTP也好，Thrift也好，总有一款适合你。为了保证实时系统通过事件驱动，我们不能让报表模块去轮询订单模块的接口，而应该反过来，在订单模块完成订单支付回调逻辑后，异步调用报表模块的接口发送通知。\n\n咋一看似乎很完美。可是……对，总有可是。想象我们不止一个模块需要下单动作的通知，比如业务监控系统想根据每小时的支付订单数来做环比告警，推荐系统想根据下单信息做实时推荐。嗯，几个模块要数据也不坏，都来订单模块注册一下，我挨个给你们发通知。随着模块增多，整个系统里终于出现了N个模块，相互调用API发送N<sup>2</sup>级别的消息通知。灾难性的架构。\n\n更不必说，如果某个下游模块的API存在无法建立连接或接口调用失败的情况，订单模块是不是居然还要做失败重试？订单模块也很委屈，这也不是我份内的逻辑。通知发送成功不成功，毕竟并不影响到订单本身的处理。\n\n对！既然是通知，显然，我们需要一个消息队列。订单模块只要往特定的队列里发送通知，任务完成。关心这条消息的模块，各自去订阅这个队列，收到消息后自行完成本身的处理逻辑，责任划分变得更加清晰。\n\n比较成熟的消息队列，包括`Kafka`, `RabbitMQ`等，这里为了简便起见，我们以更为轻量的`Redis` `Pub/Sub`功能来做功能实现，代码可以参见`feature/v0.2_pubsub`分支。\n\n修改后的订单模块回调接口如下，请求结束后，不再去访问报表模块的WebSocket客户端列表，而是将消息直接发送到Redis：\n\n``` python\nclass ApiHandler(web.RequestHandler):\n    \"\"\"订单中心支付宝支付成功回调接口\"\"\"\n    redis_cli = redis.StrictRedis.from_url(REDIS_URI)\n\n    @web.asynchronous\n    def get(self, *args):\n        # 回调逻辑，数据库写入\n        order_id = int(self.get_argument(\"id\"))\n        order_amount = int(self.get_argument(\"amount\"))\n\n        new_order_flag = False\n        if session.query(Order.id).filter_by(id=order_id).scalar():\n            # 支付成功重复回调的情况，严谨起见还应该处理回调信息不一致的情况\n            print(\"重复回调\")\n            pass\n        else:\n            # 增加订单记录；现实世界中，这里通常是改变订单的状态\n            session.add(Order(id=order_id, amount=order_amount))\n            session.commit()\n            new_order_flag = True\n        self.finish()\n        # 请求完成后，异步通过ws发送消息到客户端\n        if new_order_flag:\n            # 这里更常见的做法是，只发送增量数据，由消息订阅方自行聚合\n            data = {\"cnt\": session.query(Order).count(), \"amount\": session.query(func.sum(Order.amount)).scalar()}\n            self.redis_cli.publish(ORDER_CHANNEL, json.dumps(data))\n```\n\n报表模块的`SocketHandler`，则将WebSocket客户端列表变为自己的私有变量（我知道，在Python里没有真正的私有变量，两个下划线也不行，但你明白这个意思的，对吧），不和外部共享。redis_listener方法，最终以一个单独的线程启动。Websocket连接建立时，也不再需要查询订单模块的数据库，而是本地保存了最新的消息状态。\n\n```python                \nclass SocketHandler(websocket.WebSocketHandler):\n    \"\"\"报表系统websocket连接\"\"\"\n    redis_cli = redis.StrictRedis.from_url(REDIS_URI)\n    # 报表系统所有链接ws的客户端\n    _rpt_ws_cl = []\n    _latest_msg = json.dumps({\"cnt\": 0, \"amount\": 0})\n\n    def check_origin(self, origin):\n        return True\n\n    def open(self):\n        if self not in self._rpt_ws_cl:\n            self._rpt_ws_cl.append(self)\n            self.write_message(self._latest_msg)\n\n    def on_close(self):\n        if self in self._rpt_ws_cl:\n            self._rpt_ws_cl.remove(self)\n\n    # 报表系统订阅Redis信息\n    @classmethod\n    def redis_listener(cls):\n        ps = cls.redis_cli.pubsub()\n        ps.subscribe(ORDER_CHANNEL)\n        for msg in ps.listen():\n            if msg[\"type\"] == \"message\":\n                cls._latest_msg = msg[\"data\"]\n                for c in cls._rpt_ws_cl:\n                    c.write_message(msg[\"data\"])\n```\n\n这个阶段的架构图如下所示：\n\n<img src=\"https://raw.githubusercontent.com/reata/real-time-dashboard-example/feature/v0.2_pubsub/architecture.png\" width=\"100%\" height=\"100%\">\n\n尽管在我们的示例代码中，两个模块的代码写在一个tornado实例中。但由于它们通过消息队列进行了解耦，相互不共享其它内容，他们是可以做到单独发布的。并且想象中作为消息订阅方的业务监控系统、推荐系统，也都可以非常容易地加入到这个架构中来。\n\n消息队列在这里，就是`事件驱动架构`中常常提到的`事件总线 Event Bus`或`消息总线 Message Bus`的核心内容。\n\n但故事到这里并没有结束，可能你已经发现了，我们的架构里仍然有不合理的地方，订单模块发送的消息，是查询数据库聚合好的全量结果。可是，想象中的业务监控系统、推荐系统显然会有不同的聚合或者更为复杂的数据处理方式，作为生产者的订单模块是不可能一一为其定制化处理的。并且查询全量数据对订单模块来说本身也是一种负担，它不应该做任何聚合，而是应当增量地发送当前订单的信息，由消费者自行聚合数据。\n\n## 增量消息\n\n当消息生产方开始发送增量数据，消费方的实时计算就成为了一个问题。消息量开始变大，这要求队列有更强的吞吐能力，消费者的速度也需要跟上，多线程消费是必然的选择，可能处理故障切换上需要稍稍花一点心思，花些时间也总能搞定。但也许有一天，单机的性能达到了瓶颈，走向分布式的消费，这个时候就必须要上实时计算框架了。\n\n以保证巨大的消息吞吐量和复杂的实时计算为前提，通常这个技术栈就变成了以`Kafka`为消息队列，`Spark Streaming`或`Storm`进行分布式实时计算。实时聚合好的数据，再通过`Redis` `Pub/Sub`，或其他方案推送到数据库。\n\n另外，对于增量消息本身来说，其实不限于系统本身发送的消息。你完全可以通过订阅数据库日志，比如MySQL的binlog来达到同样的目的，不过这样需要处理数据库表结构变更时日志格式变更的问题。为了将来实时数据管道的演进（参见下一节），这里不推荐使用数据库日志作为消息源。\n\n由于涉及到的基础组件较多，就不在实例代码中进行演示了，不过相应修改过的代码注释，可以参见`feature/v0.3_real-time_computing`这个分支。这个时候，我们的架构变成了以下：\n\n<img src=\"https://raw.githubusercontent.com/reata/real-time-dashboard-example/feature/v0.3_real-time_computing/architecture.png\" width=\"100%\" height=\"100%\">\n\n之前设想的业务监控系统、推荐系统，可以分别提交各自的实时计算作业到实时计算平台，消费同一队列。对于某些不需要实时计算的模块，比如有一个短信模块，需要在支付成功后给用户发确认短信，它还是可以直接订阅Kafka消息队列（通常不会是同一个Topic）。\n\n至此，关于消息消费方的改造，我们可以说是取得了非常满意的进展。不过……你一定已经听出了言外之意，来到这个节点，我们对消息生产方有了更高的要求。\n\n作为一个数据挖掘人员，我们总是拼命地拼命地想对用户的了解再多那么一点点。想要更多的了解，当然需要更多的数据。比如今天我想知道当天登录我们应用的用户数量，这个数据可能来自用户模块。明天我想知道，用户访问我们的商城进行了商品搜索，今天热搜的词是哪些，最常被搜索到的商品又是哪几款，这个数据可能来自商品模块。所以每当有一个需求，我就需要找到对应的业务系统，请求他们把数据发送一份到消息队列。\n\n作为业务系统的开发人员，这个事件数据的需求做不做，其实一点也不影响我的系统功能，没有它业务也是正常运转的。而且很多数据发布出来，很可能只是为你一个数据系统服务。发布的消息多了，重构时代码里这些消息发送的逻辑，我也不知道还要不要维护。\n\n两难！怎么办，这个时候我们可能需要一个统一的用户行为事件管理收集模块。在不侵入业务系统代码的情况下，把用户事件数据发布出来。\n\n## 网关\n\n我们都用过Nginx，作为反向代理，Nginx负责将客户端的HTTP请求，转发到对应的业务系统。Nginx的访问日志类似以下结构：\n\n```\n127.0.0.1 - - [26/Apr/2017:15:20:05 +0800] \"GET / HTTP/1.1\" 200 396 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36\"\n```\n\n它记录了访问客户端的IP，访问事件，请求方法，请求路径等信息。很大一部分用户行为，都会伴随着客户端向服务端发送HTTP请求。也就是说，这些行为，在发起到结束期间内的某个时刻，都会被Nginx忠实地记录下来。这让我们开始思考，我们能不能做一个模块，或者说我们能否改造Nginx，让所有的流量都流经我们这个中心模块，在这个模块之上，我们可以获得更多的业务信息，比如这个用户究竟是谁，而不仅仅是一个IP；用户请求的路径，到底对应着什么行为；面对用户的请求，对应的模块返回了什么样的数据给用户？\n\n对，这就是API网关。业界比较知名的开源网关项目包括在Nginx之上基于Lua开发的[Kong](https://getkong.org/)，Netflix开源基于Java开发的[Zuul](https://github.com/Netflix/zuul/wiki)。网关作为流量的统一入口，可以做的事非常多，包括用户认证、安全监控、流量控制、缓存、日志记录、请求响应改造等等。对于数据人员来说，你只需要知道，它可以截获所有基于HTTP请求的用户行为，这就够了。\n\n代码里使用BaseHandler来模拟网关，所有的HTTP请求Handler只要继承BaseHandler，就会在响应结束后，将相应的请求-响应数据发布到Kafka。代码参见`feature/v0.4_api_gateway`分支。最终的架构如图所示：\n\n<img src=\"https://raw.githubusercontent.com/reata/real-time-dashboard-example/feature/v0.4_api_gateway/architecture.png\" width=\"100%\" height=\"100%\">\n\n至此，我们将大数据实时系统和业务系统做了最大努力的解耦。业务系统在这里的意思是，没了它，公司的产品就立刻陷入瘫痪的系统，比如不能下单，比如无法查看商品列表。而我们的数据模块，说到底属于一个决策支持系统。如果你们公司有非常厉害的决策层，根据自己的商业直觉（传说中的Business Sense）或者一线调研，总能做出正确的商业判断，这套系统就可有可无。但通常来说，你很难碰上这么厉害的人，这种时候，相信数据会让你对自己的判断多一点点自信。（当然有时候你的论点也不必这么有数据支持，吵架嘛，最讲究的是气势）\n\n对，这是最终了。Hooray！\n\n## 最后的反思\n\n回到原点，我在反思，我们真的那么需要实时数据吗。演变到最后的这个系统，对于绝大多数永远成为不了巨头的公司来说，是不是已经成为了屠龙之技？可能Monolithic时代的实时架构，结合T+1的数据挖掘分析，很多情况下就已经足够了。符合业务场景的架构，才是好的架构。\n\n但不管怎样，只是从技术的角度来看，这一路走来，很有趣，对吗？不有趣，为什么要写代码呢？\n\n## 参考阅读\n\n1. [Billions of Messages a Day - Yelp's Real-time Data Pipeline](https://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html)\n2. [Event-driven Microservices Using RabbitMQ](https://runnable.com/blog/event-driven-microservices-using-rabbitmq)\n3. [Event Driven Architecture – The Basics](https://cloudramblings.me/2015/03/31/event-driven-architecture-the-basics/)\n4. [WebSocket 教程](http://www.ruanyifeng.com/blog/2017/05/websocket.html)\n","frontmatter":{"date":"2017-06-28","excerpt":"拥抱事件驱动","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#a8d8f8","images":{"fallback":{"src":"/static/30aeffb2e86e2715e554ca3c327b9c49/31355/message_bus.jpg","srcSet":"/static/30aeffb2e86e2715e554ca3c327b9c49/b790f/message_bus.jpg 175w,\n/static/30aeffb2e86e2715e554ca3c327b9c49/43ce5/message_bus.jpg 350w,\n/static/30aeffb2e86e2715e554ca3c327b9c49/31355/message_bus.jpg 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/30aeffb2e86e2715e554ca3c327b9c49/03bd4/message_bus.webp 175w,\n/static/30aeffb2e86e2715e554ca3c327b9c49/4f28c/message_bus.webp 350w,\n/static/30aeffb2e86e2715e554ca3c327b9c49/e0825/message_bus.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":1600,"height":1099.4285714285713}}},"title":"从WebSocket说起，实时数据系统演进","slug":"/blog/to-build-a-real-time-system-starting-with-websocket/"}},{"rawMarkdownBody":"\n## 缘起\n\n做数据的人，都离不开宿命的SQL。从Hive到Spark SQL，SQL形式的ETL已经覆盖了大数据离线计算的绝大多数场景。而Presto甚至提出了SQL on everything\n的口号，旨在做一个针对异构系统的统一操作引擎，查询语言毫无疑问是SQL。而随着Flink的普及，我们也开始看到SQL进入流式计算的领域。无处不在的SQL，\nSQL可能会比我们活得都长。但即便如此，这些依然改变不了SQL本身层次多、冗余、难读的特点。\n\n我经常听数据仓库工程师说，分析人员抛过来了一个上千行、业务价值满满、同时也令人绝望的SQL。梳理清楚哪些表之间在做Join，结果又写进了哪张表，\n就需要花很长的时间，更不要说深入到其中的业务逻辑。从数据平台的角度，SQL血缘也有非常广泛的用途，首当其冲自然是构建表之间的数据血缘关系，\n数据血缘可以帮助开发人员更好地认识表之间的数据流向，理解业务，并且在需要做数据变更时去做影响分析。而在任务调度时，也可以通过表级别的血缘关系，\n反过来去推荐、诊断、优化任务级别依赖关系的配置。\n\n需求始终存在，但当年的我，不懂编译器（compiler）、语法分析（parser）、词法分析（lexical analyzer）等等概念，我试图用正则表达式（regular expression）\n这个朴素的手段来解决问题，想法也非常直接，FROM或者JOIN后面就是源表，INSERT INTO/INSERT OVERWRITE TABLE后面就是目标表嘛，easy，这个我稍微调试一下，\n两下就能写出来。\n\n```python\n# 当年我用来抓取表名的正则\nsource_table_regex = re.compile(r\"(?:from|join)\\s+(\\S*)(?:\\s+|;)\", re.IGNORECASE)\ntarget_table_regex = re.compile(r\"insert\\s+(?:into|overwrite)\\s+table\\s+(\\S*)\\s+\", re.IGNORECASE)\n```\n\n这个方法显然不是这么准确，比如碰到下面这个SQL\n\n```sql\ninsert overwrite table table_foo\nselect * from /* let's play with a comment */ table_bar;\n```\n\n显然我会认为/*是表名，虽然人家不过是个注释。\n\n再比如说我还可以写个恶趣味的SQL\n\n```\nselect * from table_foo\nwhere description = \"from Excel\";\n```\n\n这时候Excel也变成一个表名了，确实很表，但它不是我想要的。\n\n虽然看起来确实很傻瓜、充满了漏洞，但这个正则的方案还是活了很久，事实上，加上一些if-else的判断和预处理，在生产环境中可能80%~90%的情况，\n它是可以返回正确结果的。不过，作为一个有追求的程序员，怎么可以满足于90分对吧。我也想借此机会了解一下编译原理相关的东西，\nAST抽象语法树这个东西我听的够多了，它到底是个啥呢？这些都一起促成了做[SQLLineage](https://github.com/reata/sqllineage)这个项目的初衷。\n\n（2022.3，作者按：学过编译原理之后回头看，词法分析器Lexer用的形式语言就是正则哦。）\n\n## 设计思路\n\n在动手开发之前，我先有过一些思考：\n\n我希望这个工具尽量简单，有命令行的界面，同时作为一个Python Package发布，这样pip安装后也可以import到其他的项目中。这个简单的工具，不会去执行SQL，\n或者连接任何元数据系统（比如Hive metastore）。我只对代码进行静态分析。优点当然是简单，而劣势就是可能这个血缘解析只能完美地做到表级别的血缘，\n对于字段级别的血缘，假如有select *的情况，就会丢失信息。这也是我暂时没有做字段级别血缘的主要原因。\n\n（2022.3，作者按：字段级别的血缘已经有初步的支持啦，请看[SQLLineage v1.3：迈向字段血缘](/blog/sqllineage-towards-column-lineage/)）\n\n另外一个取舍是从原理层面来说，我不想自己去做parser，最理想的情况是有一个通用的parser，可以将绝大多数常见的SQL方言解析成AST，\n然后我只要对AST进行分析，产出源表、目标表等人类可读的结果就好。遵循这个思路，我找到了[sqlparse](https://github.com/andialbrecht/sqlparse)这个项目，\n它将自己定位为一个不进行语法校验的通用SQL parser，当然也是基于Python来开发的。\n\n在sqlparse把SQL语句解析成AST之后，我们要做的事情就和之前正则的时候大差不差了。对于源表，无非是找到FROM或者JOIN相关的Keyword，看它后面的\nToken是什么，如果是Comment或者Whitespace就继续往后看，如果是Identifier，取出它的name，如果是Subquery，则递归地再搜索一层。当然如前面所说，\nsqlparse不进行语法校验，一方面这意味着有了更多SQL方言支持的可能，当然反过来，解析出来的结果可能是不对的。比如Subquery，在没写alias的情况下\n（在某些SQL方言中，子查询不写别名，语法也是正确），会被解析成了Identifier，不能直接取name。真正开始写代码之后，我发现各种各样的边界条件都是可能存在的。\n\n所以最开始在我的设想中，这个package就需要充分的测试，尽可能广的测试用例，尽可能高的测试覆盖。我是用pytest框架来做单元测试的，目前1.0版本总共有77个\n测试用例，代码覆盖率是97%，可以在[codecov](https://codecov.io/gh/reata/sqllineage)看到详尽的代码覆盖情况。\n\n## Show Me The Code\n\nsqllineage已经发布到了PyPI上，可以通过pip直接进行安装使用：\n```bash\n$ pip install sqllineage\n# 安装完成之后，会自带一个sqllineage的命令行工具\n$ sqllineage -e \"insert into db1.table1 select * from db2.table2\"\nStatements(#): 1\nSource Tables:\n    db2.table2\nTarget Tables:\n    db1.table1\n```\n\n安装后也可以在Python脚本中直接调用：\n```python\nfrom sqllineage.runner import LineageRunner\nsql = \"\"\"insert into db1.table11 select * from db2.table21 union select * from db2.table22; \ninsert into db3.table3 select * from db1.table11 join db1.table12;\"\"\"\nresult = LineageRunner(sql)\nprint(result)\n# 打印result，会产出下面的信息\n# Statements(#): 2\n# Source Tables:\n#    db1.table12\n#    db2.table21\n#    db2.table22\n#Target Tables:\n#    db3.table3\n#Intermediate Tables:\n#    db1.table11\n\n# 也可以直接获取各个源表\nfor tbl in result.source_tables: \n    print(tbl)\n#db1.table12\n#db2.table21\n#db2.table22\n\n# 目标表当然也是可以的\nfor tbl in result.target_tables: \n    print(tbl)\n# db3.table13\n\n# 甚至还可以调用matplotlib绘制血缘图\nresult.draw()\n```\n下面这样一张表级血缘图会自动弹出：\n\n![sqllineage](../images/sqllineage.png)\n\n\nSQLLineage目前已经发布v1.0版本，进入稳定可用状态，让我一起来探索SQL血缘吧！\n\n（2022.3，作者按：从v1.2.2版本起，SQLLineage可以部署为一个Web App，欢迎访问这个[DEMO](https://sqllineage.herokuapp.com/)）\n\n\n## 相关链接\n- 代码：[https://github.com/reata/sqllineage](https://github.com/reata/sqllineage)\n- 文档：[https://sqllineage.readthedocs.io/](https://sqllineage.readthedocs.io/)\n- PyPI：[https://pypi.org/project/sqllineage/](https://pypi.org/project/sqllineage/)\n","frontmatter":{"date":"2020-09-20","excerpt":"构建表级血缘","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e29ed1737d1b33312665d4f1739cee79/f6810/dag.jpg","srcSet":"/static/e29ed1737d1b33312665d4f1739cee79/eee8e/dag.jpg 400w,\n/static/e29ed1737d1b33312665d4f1739cee79/1e21a/dag.jpg 800w,\n/static/e29ed1737d1b33312665d4f1739cee79/f6810/dag.jpg 1600w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/e29ed1737d1b33312665d4f1739cee79/dff21/dag.webp 400w,\n/static/e29ed1737d1b33312665d4f1739cee79/b2a35/dag.webp 800w,\n/static/e29ed1737d1b33312665d4f1739cee79/e9d78/dag.webp 1600w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":900}}},"title":"SQLLineage: 基于Python的SQL血缘分析工具","slug":"/blog/sqllineage-a-sql-lineage-analysis-tool/"}},{"rawMarkdownBody":"\nPython的编码问题由来已久，相信每一个Python程序员，都对UnicodeDecodeError和UnicodeEncodeError再熟悉不过了，这种时候就需要通过对变量进行encode或者decode，转换为对应的数据类型。是使用encode还是decode，就成为了一个问题。如果你不想每次只是来回地尝试，那么掌握一些原理想必是有益的。\n \n在Python 2中，`str`和`unicode`两种数据类型，都可以用来表示字符串。`str`实际上是“字节串（byte string/sequence of bytes）”，它表示使用某种特定编码后的一组字节序列，而这个特定编码，在Python 2里，默认是ASCII编码。而`unicode`则如其名，表示的是\"Unicode字符\"，因而它不表示任何特定编码（真的不表示任何编码吗？请继续往下读）。在Python 2里，绝大多数函数都接受两种字符串类型中的任意一种，`unicode`和`str`甚至可以直接比较是否相等，它们都是一个公共父类`basestring`的子类。在Python2中，要创建一个`str`，可以使用自带函数`str()`，或者字符串字面值（string literal），比如`mystring = 'This is my string.'`。而要创建一个`unicode`，可以使用自带函数`unicode()`，或者带u前缀的字符串，比如`my_unicode = u'This is my Unicode string.'`\n \n在Python 3中，只有唯一的一种字符串类型`str`，它是Unicode。“字节串”如今用`bytes`数据结构来表示。普遍来说，接受`str`类型的函数，通常不再支持传`bytes`进去，两种类型也不再可以直接比较是否相等。Python解释器甚至有命令行的flag可以选择性地将混用`bytes`和`str`作为异常进行抛出。\n \n这是很好的设计选择，Unicode是正确的字符串类型。而作为一种副作用，`bytes`类型在需要使用的时候，也远比Python 2中的`str`更为好用。\n \n此外，Python 3还在3.3这个版本中，引入了又一个不那么为人所知，但同样重要的变化。我们会详细讨论这个变化，但首先，我们需要一些背景知识。\n\n## 字符集与编码集\n\nUnicode不是一种编码，这是需要首先厘清的一个概念。你可以说，一个字符串是ASCII编码的、GBK编码的、GB2312编码的，UTF-8编码的，这里列举的都是编码类型，但Unicode不是。Unicode本质上是一个字符集，可以想象为一个非常大乃至无所不包的字符（character）数据库，每个字符都对应着一个唯一的id，但如何将这个字符对应的id转换为一个字节串，换句话说就是一串计算机能理解的0和1，这个不属于Unicode考虑的范畴。\n\n而深究起来，Unicode所包含的最基本单元，也不是字符（character），而是码位（code point）。如果要把一个Unicode码位在屏幕中打印出来，你会发现码位和我们平时认知的字符，并不存在清晰的对应关系。有时一个字符可能就对应一个码位，而有时一个字符可能由多个码位组成，甚至有时候，看起来多个字符对应的是一个码位。\n\n下面是一些例子：\n- 一一对应：码位`U+0041`，对应[LATIN CAPITAL LETTER A](http://www.fileformat.info/info/unicode/char/0041/index.htm)，打印出来显示为A。\n- 一个字符对应多个码位：码位`U+0327`，对应[COMBINING CEDILLA](http://www.fileformat.info/info/unicode/char/0327/index.htm)。码位`U+0063`，对应[LATIN SMALL LETTER C](http://www.fileformat.info/info/unicode/char/0063/index.htm)。将这两个码位组合起来，可以得到字符ç。而这个字符也可以用单一的码位`U+00E7`来表示，对应[LATIN SMALL LETTER C WITH CEDILLA](http://www.fileformat.info/info/unicode/char/00e7/index.htm)。这里`U+00E7`在Unicode中称为“合成形式”，`U+0063 U+0327`称为“分解形式”。Unicode定义了将彼此等价的序列转成同一序列的Unicode正规化规则。\n- 看起来一个码位对应多个字符：码位`U+FDFA`，对应[ARABIC LIGATURE SALLALLAHOU ALAYHE WASALLAM](http://www.fileformat.info/info/unicode/char/fdfa/index.htm)，打印出来显示为ﷺ。按照Unicode的分解规则，这个码位可以分解成接近20个“字符”。因为使用较为频繁，Unicode将其表示为一个码位。\n\nUnicode当前是按照字符平面（plane）来组织码位的。每个字符平面包含 2^16 = 65535 个码位。目前Unicode总共定义了17个字符平面，其中0号平面（U+0000 - U+FFFF）涵盖了绝大多数自然语言的码位，而（U+4E00 - U+9FFF）是中日韩同一表意文字所在区间，常用的中文字符都在这个区间内。\n\n所以Unicode是字符集，或者更严格地说是码位集，它规定了一个码位的二进制代码，但没有规定这个二进制代码应该如何存储。比如汉字[“杭”](https://www.fileformat.info/info/unicode/char/676d/index.htm)，对应的码位`U+676D`，也就是十进制的`26477`（int('676D', 16) == 26477），二进制的`0b110011101101101`（bin(int('676D', 16)) == '0b110011101101101'; int('0b110011101101101', 2) == 26477）。这个二进制数有15位，这意味着存储该二进制数至少需要15位，也即至少需要两个字节。\n\n假设我们就用二进制代码来存储Unicode（这也是ASCII的思路），\"杭\"的二进制代码是15位，对其首位补0，得到一个16位的二进制数，'01100111:01101101'。这里引入了一个严重的问题，计算机怎么知道在这里两个字节表示一个码位，而不是两个字节表示两个码位。按两个码位来理解，对计算机完全行得通。从左往右，第一个字节，'01100111'，十进制的103，对应Unicode码位'U+0067'，也就是英语字母g。第二个字节，'01101101'，十进制的109，对应Unicode码位'U+006D'，也就是英语字母m。这样计算机无从分辨。\n\n假设稍作修改，我们要求，所有Unicode都表示为两个字节，不足两个字节的首位补0。那么，ASCII编码所能表示的英文字母和符号，第一个字节都是0。两个字节只有65536个码位，Unicode的数量是远大于这个数字。我们需要三个字节，才能保存现有的所有字符。计算机对于2的幂指数来说更为优化，于是我们采用四个字节，来表示所有Unicode。这基本上就是UTF-32编码（Unicode Transformation Format, 32-bit）。很显然，绝大多数字符都不需要四个字节的空间，所以UTF-32编码多少显得有些浪费存储空间。\n\nUTF-32的另一个问题是，定死了四个字节数后，就意味着同时限制了Unicode的扩展性，使得Unicode变成了和ASCII一样字符编码统一的集合。ASCII只用一个字节，只能表示256个码位。Unicdoe用四个字节，可以表示2^32 = 4294967296个码位。从目前Unicode的实际分配情况来说，这个空间是远远满足需求的。但是，IPv4发明的时候，谁也想不到有一天我们会需要IPv6，对吗？\n\nUTF-32有时也被称为UCS-4（“Universal Coded Character Set, 4-type”），UCS是一种类似Unicode的ISO/IEC标准，和Unicode共享码位的分配，但不包含Unicode定义的属性和规则。之前，当Unicode的码位数量还没有超过2^16=65536时，另外还有一种编码叫做UCS-2，每个码位表示为两个字节（也即16个二进制数）。但时至今日，Unicode的范围已经超过了16个二进制数所能表述的区间（16个二进制数，恰好是0号平面所在的区间，目前又引入了其它16个平面，但不是每个平面已经分配了码位），所以UCS-2不再能正常使用了。但通过一些简单的修改，有时将码位表示为两字节、有时表示为四字节，就得到了UTF-16（Unicode Transformation Format, 16-bit）编码。\n\nUTF-16的原理如下：\n- 如果码位可以用16个二进制数表示，那么就直接表示为16个二进制数\n- 如果不能，……\n\n为了进一步压缩存储空间，又引入了UTF-8编码，UTF-8也是一种变长编码。视Unicode的码位大小，编码后的字节串，可能是一字节、二字节、三字节、四字节不等。具体如下：\n- ……\n\n在现实中，UTF-8和UTF-16是最为常见的Unicode编码规则。Windows的大部分API都使用UTF-16，而Unix/Linux系统大多使用UTF-8。\n\n## 参考阅读\n\n1. [How Python does Unicode](https://www.b-list.org/weblog/2017/sep/05/how-python-does-unicode/)\n2. [字符编码笔记：ASCII，Unicode 和 UTF-8](http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html)\n","frontmatter":{"date":"2018-07-02","excerpt":"Python是如何处理Unicode的","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/21424970c3fcc547303fc08274927ef1/f2f90/decode_encode.png","srcSet":"/static/21424970c3fcc547303fc08274927ef1/6addd/decode_encode.png 400w,\n/static/21424970c3fcc547303fc08274927ef1/fd8e4/decode_encode.png 800w,\n/static/21424970c3fcc547303fc08274927ef1/f2f90/decode_encode.png 1600w,\n/static/21424970c3fcc547303fc08274927ef1/cad54/decode_encode.png 3200w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/21424970c3fcc547303fc08274927ef1/f5c71/decode_encode.webp 400w,\n/static/21424970c3fcc547303fc08274927ef1/781f0/decode_encode.webp 800w,\n/static/21424970c3fcc547303fc08274927ef1/35146/decode_encode.webp 1600w,\n/static/21424970c3fcc547303fc08274927ef1/36e25/decode_encode.webp 3200w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":1199}}},"title":"Encode还是Decode，这是一个问题","slug":"/blog/how-python-does-unicode/"}},{"rawMarkdownBody":"\n什么是日志？在非常长的一段时间内，日志对我来说，就是协助定位系统问题的一种手段。在开发阶段我打了非常多DEBUG级别的日志协助调试，到了线上，我们只保留INFO级别以上的日志。这样，在系统出现和我们预期不符的状况时，通过日志，我可以定位到异常发生时的一些蛛丝马迹。这就是日志所有的作用了。tailf几乎是我会对日志去做的唯一操作。\n\n所以，当来到大数据领域，我听到人们都在说，我们需要一套日志系统，我一直很迷茫。[ELK](https://www.elastic.co/cn/products)可能是最容易搭建的日志系统。`Logstash`抽取硬盘上的日志文件，发送到`Elastic Search`，最后通过`Kibana`进行可视化。但我所见过的`ELK`实践，依然没有改变那个最初的定位，我们还是在通过日志系统查询系统异常（Exception）。这和大数据又有什么关系？\n\n某种程度上，日志系统也成为了像大数据概念本身一样的[Teenage Sex](https://whatsthebigdata.com/2013/06/03/big-data-quotes/)。所有人都在谈论它，可是没有太多人知道它究竟是什么。\n\n直到有一天，我们读写分离的系统遭遇了严重的主从不同步问题，我好奇地想搜索一下MySQL的主从同步究竟是通过什么机制来实现的，我第一次知道了MySQL的**binary log**这个东西，从此打开了新世界的大门。\n\n## 日志与表合二为一\n\n更详细的介绍可以参见这篇[博客](https://www.percona.com/blog/2013/01/09/how-does-mysql-replication-really-work/)，但简单地说，主实例的所有INSERT和UPDATE操作事件，都会被发布到binlog中，而从实例只要同步这个日志并尽可能快地执行日志中的变更事件即可。主从同步延迟问题的排查也就变得容易入手，从实例或者无法及时读取binlog，或者读取后无法及时执行。反过来说，只要从实例严格执行了主实例发布的所有事件日志，主从一定处于同步状态。\n\nbinlog设计之初，就是供机器阅读的，它忠实记录了发生的事件和时间。把它看做一个数据结构，binlog只允许唯一一个操作，那就是**追加 append**。依次执行所有这些事件，就能来到最新的状态。就像只要知道所有的交易记录，我总能知道你的账户余额。\n\n数据操作事件Log和数据库表Table，这两个概念在这里奇妙地合二为一。甚至Log比Table更棒，Table永远只有最新的状态，而通过Log，我可以来到任何中间状态。\n\n## 日志系统才应该是数据的核心，而不是数据仓库\n\n这里我们要稍微绕些路，谈一谈数据仓库。来到一定规模的公司，数据仓库的建设会被提上议程。不管是基于`Oracle`的传统数据仓库，还是`Hadoop`时代以`Hive`为中心，配合`Presto`，`Impala`等OLAP工具的分布式数据仓库，其特点并没有发生本质的变化。数据仓库将散落（微服务时代，数据实际上变得更加散落了）、来源各异（异构数据库，搜索引擎）的数据，集中到统一的地方，配合ETL过程，将数据整理成更加规范的形式，供报表、分析、挖掘使用。事实上，所有花哨的名词，数据挖掘、机器学习、神经网络，如果缺乏统一的数据接入层，都只是花哨的名词，而已。\n\n数据仓库的这些特点，使其非常适合成为整个企业数据的核心，毕竟所有的数据经过汇总都可以从仓库中方便地获取。但有一点不该忘记，数据仓库的定位，始终是离线的，它定时地将数据从相互隔离的数据源中抽取过来，转换加工后，加载到目标系统。当你开始搭建实时数据的时候，数据仓库的核心地位，就开始成为阻碍因素。并不是所有数据消费系统，都可以接受T+1的时滞。\n\n我们注意到大数据平台，和分布式数据仓库（基本上就是`Hive`）的概念是不划等号的，它通常还包括实时计算的部分。而在上一节里我们已经发现了，作为离线的表和作为实时的日志，这两个概念是一体两面。如果你拥有实时，你就拥有了离线！（反过来却不是）\n\n而实时数据的管理？不言而喻，是日志系统。\n\n## 结构化的日志数据\n\n数据仓库的数据都非常结构化，易于查询。而日志系统的数据，都是日志，日志通常被视为半结构化的数据，它本身具备一定的结构，但是又以非强制约束的形式存储。如果我们的日志本身就是格式化的，无疑在易用性上会更胜一筹。\n\n所谓结构化，千万不要认为JSON是格式化的。我们需要数据带有schema，给定一份数据，我想清楚地知道它有哪些字段，每个字段是什么类型。\n\n在日志结构化方面，[Avro](https://avro.apache.org/), [Thrift](https://thrift.apache.org/), [Protocol Buffer](https://developers.google.com/protocol-buffers/)都是不错的选择。如果业务系统的数据交互，已经在使用这样的格式。或者说，对于HTTP API，你使用了诸如[SWAGGER](https://swagger.io/)这样的API工具来规范数据传输。那么打造这样一个日志系统，会容易得多。\n\n## 最后\n\n以日志为核心，`Data Pipeline 数据管道`这个词可能前所未有地接近其字面意思。毕竟，不能简单地插拔、组合，还怎么能称之为管道呢？\n\n## 参考阅读\n\n1. [The Log: What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)（LinkedIn工程师详解Kafka的前世今生，推荐必读）\n2. [How does MySQL Replication really work?](https://www.percona.com/blog/2013/01/09/how-does-mysql-replication-really-work/)\n","frontmatter":{"date":"2017-07-01","excerpt":"只是人类可读的DEBUG信息吗","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/3f96ba2fdd3f8d28dc882568a49738e7/f6810/log.jpg","srcSet":"/static/3f96ba2fdd3f8d28dc882568a49738e7/eee8e/log.jpg 400w,\n/static/3f96ba2fdd3f8d28dc882568a49738e7/1e21a/log.jpg 800w,\n/static/3f96ba2fdd3f8d28dc882568a49738e7/f6810/log.jpg 1600w","sizes":"(min-width: 1600px) 1600px, 100vw"},"sources":[{"srcSet":"/static/3f96ba2fdd3f8d28dc882568a49738e7/dff21/log.webp 400w,\n/static/3f96ba2fdd3f8d28dc882568a49738e7/b2a35/log.webp 800w,\n/static/3f96ba2fdd3f8d28dc882568a49738e7/e9d78/log.webp 1600w","type":"image/webp","sizes":"(min-width: 1600px) 1600px, 100vw"}]},"width":1600,"height":900}}},"title":"重新认识日志","slug":"/blog/a-review-of-log/"}},{"rawMarkdownBody":"\nGitHub的API可以说是Web API设计的典范，仅仅是阅读文档就让人获益良多，时刻为你的Web知识及HTTP协议查漏补缺。\n\nGitHub API大部分接口都需要认证，而其主要推荐的认证方式是OAuth。OAuth的接入也分好几种，根据不同的应用场景：\n\n- 如果你的应用允许/需要用户登录自己的GitHub账号\n  - 对于Web应用：[基于OAuth的Web application flow](https://docs.github.com/en/developers/apps/building-oauth-apps/authorizing-oauth-apps#web-application-flow)\n  - 对于命令行或其他设备：[基于OAuth的Device flow](https://docs.github.com/en/developers/apps/building-oauth-apps/authorizing-oauth-apps#device-flow)\n- 如果你的应用只需要单一/固定的GitHub账号的权限，比如你自己的账号：\n  - 适用于服务端：[OAuth and personal access tokens](https://docs.github.com/en/rest/overview/other-authentication-methods#via-oauth-and-personal-access-tokens)\n\n\n## Web Application Flow\n1. 应用将用户重定向到GitHub登录页，用户在GitHub上登录\n2. GitHub将用户重定向回应用站点，重定向回来时候会带上一个code。应用可以通过这个临时code，在10分钟内兑换access_token。\n3. 应用站点保存access_token，并在之后所有针对GitHub API的请求带上该token\n\n## Device Flow\n1. 命令行应用调用Github拿到认证链接和用于识别当前应用的验证码，验证码15分钟失效。\n2. 命令行应用将验证码展示给用户，并提示用户去认证链接输入该验证码。\n3. 命令行应用轮询GitHub检查用户是否已经输入了验证码（命令行授权）。用户输入成功后，即可拿到access_token。\n\n## OAuth and Personal Access Tokens\n按照[教程](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)，\n在Settings - Developer settings - personal access tokens页面自主添加即可。注意添加成功后，只有在当时token可见，\n一旦页面关闭之后就再也拿不到这个token了，一定记得保存好。\n\nPersonal Access Token相比直接用登录密码的好处在于：\n- 更细粒度的权限控制，可以勾选该Token可以访问的API，读写控制\n- Token随时可以删除，也可以设置失效时间，到期自动失效\n\n## 有了Token之后怎么用\n\n### 方案一：Basic Authentication\n```bash\ncurl -u username:token https://api.github.com\n```\n\n### 方案二：OAuth token (sent in a header)\n```bash\ncurl -H \"Authorization: token OAUTH-TOKEN\" https://api.github.com\n```\n","frontmatter":{"date":"2021-09-22","excerpt":"选择适合你的认证方式","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/b5899b93147f962a4c74c82c07471968/221c0/github_api.png","srcSet":"/static/b5899b93147f962a4c74c82c07471968/97aa7/github_api.png 169w,\n/static/b5899b93147f962a4c74c82c07471968/a9084/github_api.png 338w,\n/static/b5899b93147f962a4c74c82c07471968/221c0/github_api.png 676w","sizes":"(min-width: 676px) 676px, 100vw"},"sources":[{"srcSet":"/static/b5899b93147f962a4c74c82c07471968/60010/github_api.webp 169w,\n/static/b5899b93147f962a4c74c82c07471968/8d832/github_api.webp 338w,\n/static/b5899b93147f962a4c74c82c07471968/cae60/github_api.webp 676w","type":"image/webp","sizes":"(min-width: 676px) 676px, 100vw"}]},"width":1600,"height":991.7159763313609}}},"title":"GitHub API与OAuth认证","slug":"/blog/github-api-and-oauth/"}},{"rawMarkdownBody":"\nInstacart是一家食品类垂直电商，在Kaggle上举办的这场比赛中，参赛人员需要利用其公开的交易数据建立机器学习模型，预测用户会再度购买哪些商品。\n\n知道用户可能会购买哪些商品，有什么样的用处呢？最直接的，平台可以针对性地给用户发放促销优惠券，而更远景的目标，结合一些其它数据，或许可以做销量预测和库存优化，节约仓储、采购成本。\n\n## 已有数据集\n\n比赛使用的数据仅包含交易数据，不包含浏览数据。主要由以下几张表构成：\n\n1. 订单表 orders（订单ID，用户ID，所属数据集，该用户的订单序号，订单下单在星期几，订单下单所在小时，距离上一次下单过去的天数）：数据粒度为一个订单事实。其中，所属数据集包含三类：a) 先验集：所有用户在历史一段时间内产生的所有订单；b) 训练集：从所有用户中抽出一部分训练用户，在考察周期内产生的所有订单；c) 测试集：除去训练用户外剩下的用户，在考察周期内产生的所有订单。先验集中，每个用户可能包含多个订单。而对于训练集和测试集，两者的用户无交集，且每个用户在各自集合内也只会有一个订单。\n\n2. 商品表 products（商品ID，商品名称，通道ID，分类ID）：数据粒度为一件商品。\n\n3. 通道表 aisles（通道ID，通道名称）：数据粒度为一个通道。这里的通道，就是超市里的通道/走道，每一个通道两侧的商品，通常是一个类别的，而走道上方，往往会有一个标识牌。\n\n4. 分类表 departments（分类ID，分类名称）：数据粒度为一个分类。是比通道更大的分类概念，但二者相互没有确定的包含关系。\n\n5. 先验集订单商品表 order_products_prior（订单ID，商品ID，加入购物车的次序，是否复购）：注意先验集包含所有的用户，这里的数据粒度是历史一段时期内，所有用户购买的所有商品记录。\n\n6. 训练集订单商品表 order_products_train（订单ID，商品ID，加入购物车次序，是否复购）：这里是训练集用户，在考察期内，购买的所有商品记录。上面提过，这个数据集里的每个用户，只会有一个订单，其中包含若干个商品，可能包含也可能不包含复购的商品。\n\n如果不使用NLP的方法对名称类字段进行处理的话，商品名称、通道名称、分类名称这几个字段是没有用的。在实际项目中，也没有进行相关处理，所以后面这几个字段将略过不谈。\n\n最终产出的数据，是订单表的测试集中，每个订单所包含的复购商品，也即仅包含复购的测试集订单商品表。由于上面提到了，训练集和测试集实际上是按照用户划分的，所以最终提交的数据，也是测试用户在考察期间内，复购的所有商品。\n\n## 问题的评价标准\n\n按照每个用户实际复购的商品，和预测复购的商品，针对单个用户计算F1得分，最终再按用户人均的F1得分。\n\n复购预测是一个典型的非均衡分类问题，绝大多数的商品都会落入TN的范围内——一件商品，算法预测不复购、用户实际也没有复购，所以准确率Accuracy是一个不好的指标，预测用户不复购任何商品，这个分类问题就可以得到很高的准确率。\n\n与此同时，精准率Precision和召回率Recall也都不足以描述问题的关键。\n\n片面考虑精准率，算法会趋于谨慎，只预测非常有把握实际会复购的，从而漏掉一部分实际复购的；覆盖面不够广，促销的力度可能不够。\n\n片面考虑召回率，算法会趋于冒险，尽可能多地把实际复购的商品都挑选出来，从而预测了一部分实际上用户没有复购的；覆盖面过于广，造成促销活动的浪费。\n\nF1 = 2 / (1/Precision + 1/Recall) = 2TP / (2TP + FP + FN)\n\n注意到，在F1的计算过程中，TN是不纳入计算的。这也符合问题的初衷。\n\n## 机器学习表述\n\n将业务问题转化为一个机器学习问题的过程是项目成败的关键。从业务方提过来的问题，常常更贴近于业务现实，比如“最近公司的优惠券发得太漫无目的了，成本大幅上升，为了更高效地发放优惠券，我们希望能预知用户可能购买的商品”。而后续需要将问题进行机器学习的表述。\n\n汇总出可以取得到的相关数据（使用交易数据，浏览数据很重要但可能无法获得），梳理出问题的评价标准（使用F1，而不是Accuracy）是问题的机器学习表述中极为关键的步骤。数据决定了后续算法的潜力如何，而评价标准则决定了算法潜力兑现后，模型多大程度上解决了业务问题。\n\n而剩下的关键问题是，机器学习建模。我们面临的是哪一类机器学习问题？模型的粒度，也就是说，一条训练样本代表着什么？\n\n在梳理问题评价标准的同时，我们已经发现，这是一个分类问题。给定一个用户，一件商品，算法需要预测出，用户是否会复购这件商品，因而模型的粒度是用户-商品。而复购，意味着用户曾经购买过这件商品才可能形成复购，不需要将所有用户和所有商品做笛卡尔积，挑选出各位用户及其曾经购买过的所有商品的组合即可。\n\n注意到，在给定的数据集中，订单的粒度是一个订单，对应着一个用户的一次下单行为，订单商品的粒度是一个订单商品，对应着一个用户的一次商品购买，用户-商品组合可能出现多次。而我们的模型要求，每个用户-商品只允许出现一次，所以各类特征，不能简单从原始表中直接拿来用，而是需要根据用户-商品的粒度做一些聚合操作。\n\n## 特征工程之维度建模\n\n机器学习的模型，要求每条训练样本是用户-商品粒度，这和既有的订单表、订单商品表都不符，所以需要聚合特征。尽管不完全等同于数据仓库维度建模中的事实表和维度表区分，我们依然可以将用户在未来购买某件商品看做是一件未来的事实，围绕这个事实，添加一系列的维度，每个维度包含其自身的维度属性，用作模型的特征。具体地，整理出来的特征如下：\n\n1 用户特征：\n\n1.1 购买数次；1.2 购买频率（间隔天数）；1.3 复购率；1.4 平均每单购买的商品数；1.5 第一次购买距最后一单天数；1.6 该用户购买的总商品数；1.7 该用户购买多少种商品\n\n2 商品特征：\n\n2.1 购买订单数；2.2 购买用户数；2.3 复购订单数；2.4 复购人数；2.5 复购率\n\n3 用户商品特征：\n\n3.1 购买订单数；3.2 复购订单数；3.3 订单复购率；3.4 最近购买距今间隔（天）；3.5 最近购买距今间隔（订单）；3.6 该商品被添加到购物篮中的平均位置；3.7 该商品次数占比：该商品购买次数/该用户所有商品购买次数；3.8 最后一次购买标识；3.9 用户购买该商品的次数；3.10 用户第一次购买该商品所处的订单数；3.11 用户最后一次购买该商品所处的订单数；3.12 最近一次购买商品 - 最后一次购买该商品；3.13 该商品购买次数 / 第一次购买该商品到最后一次购买商品的的订单数；3.14 最近N单购买次数\n\n4 分类特征：\n\n4.1 购买订单数；4.2 购买用户数；4.3 复购订单数；4.4 复购人数；4.5 复购率\n\n5 用户分类特征：\n\n5.1 购买订单数；5.2 复购订单数；5.3 订单复购率；5.4 购买商品次数；5.5 复购商品次数；5.6 商品购买复购率；5.7 购买商品种类；5.8 复购商品种类；5.9 商品种类复购概率；5.10 最近购买距今间隔（天）；5.11 最近购买距今间隔（订单）；5.12 订单购买顺序统计；5.13 通道购买商品次数占比：该通道商品购买次数/所有商品购买次数；5.14 通道购买商品种类占比：购买该通道商品种类/所有购买商品种类；5.15 最后一次购买标识\n\n6 通道特征：\n\n6.1 购买订单数；6.2 购买用户数；6.3 复购订单数；6.4 复购人数；6.5 复购率\n\n7 用户通道特征：\n\n7.1 购买订单数；7.2 复购订单数；7.3 订单复购率；7.4 购买商品次数；7.5 复购商品次数；7.6 商品购买复购率；7.7 购买商品种类；7.8 复购商品种类；7.9 商品种类复购概率；7.10 最近购买距今间隔（天）；7.11 最近购买距今间隔（订单）；7.12 订单购买顺序统计；7.13 通道购买商品次数占比：该通道商品购买次数/所有商品购买次数；7.14 通道购买商品种类占比：购买该通道商品种类/所有购买商品种类；7.15 最后一次购买标识\n\n聚合出上述特征后，放到XGBoost的模型中去训练，即可得到不错的分类模型。关于特征的选择，参考了这篇[KDD论文](http://www.kdd.org/kdd2016/papers/files/adf0160-liuA.pdf)，里面有更详细的特征分析。\n\n## 算法的优化目标 vs 问题的优化目标\n\n实际上，在模型的训练过程中，有一个隐含的问题。之前提到了，这个复购预测的评价指标，是人均F1得分。而XGBoost默认的优化目标是[logloss](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood)，二者不是正相关的。换言之，是可能出现logloss优化到最小，而人均F1不是最优的情况。\n\n二分类问题，针对F1进行优化，也有非常多的研究。可以针对性地设计优化函数，也可以针对条件概率分类器调整阈值。相关的论文可阅览参考阅读，我们这里采用的是后一种方法。因为Xgboost分类的结果，是一个概率表述，默认当概率大于0.5时，分为正类，否则为反类。调整预估是正类的阈值到0.21左右，在这个阈值的情况下，人均F1得分会最大化。具体的值是通过交叉验证来确定的。\n\n## 无复购的处理\n\n由于复购预测的评价指标是人均F1得分，这样就难免出现无任何商品复购的用户，而如果算法也预测用户没有任何购买，也即所有的预测结果都是TN，那么F1的计算结果会是0/0，没有意义。\n\n为了规避上面这个问题，评价规则特别指出，如果预测用户没有任何复购，需要指明其复购商品为None。如果预测是None，实际也是None，该用户F1的得分就会是1。\n\n但这个同时滋生了另一个问题，None就成为了一个特殊的“商品”。根据目前的用户-商品模型，用户无复购，即“购买了None”的概率P(None) = 1 - P(复购商品1)...P(复购商品n)。也就是说，如果算法预测用户没有复购任何商品，这是用户就购买了None。\n\n社区里有人指出，在目前评价标准为人均F1的情况下，针对None单独做一个预测模型，最终的效果会更好。这个策略我没有进行尝试。可以参考[Instacart Market Basket Analysis, Winner's Interview: 2nd place, Kazuki Onodera](http://blog.kaggle.com/2017/09/21/instacart-market-basket-analysis-winners-interview-2nd-place-kazuki-onodera/?utm_source=Mailing+list&utm_campaign=3832403754-Kaggle_Newsletter_09-10-2017&utm_medium=email&utm_term=0_f42f9df1e1-3832403754-399140641)。\n\n## 更高阶的玩法\n\n在参与这项比赛的过程中，我就察觉出来了，我基本上一直在搞特征工程——想特征，交叉验证测试，筛选特征。这个多少让数据科学变得不那么性感。\n\n但是，比赛结束后，果然，有更性感的方案。最终排名第三的用户，公布了他的神经网络算法，所有特征是通过神经网络自动生成的，之后再用XGBoost跑分类模型。神经网络大法好！\n\n有兴趣可以参考[3rd-Place Solution Overview](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38097)。\n\n## 结语\n\n我的模型，最终在所有2623名参赛选手中，排名662，前26%，未获得奖牌。接下来，还需要继续努力。Go Kaggle!\n\n## 参考阅读\n\n1. [Repeat Buyer Prediction for E-Commerce](http://www.kdd.org/kdd2016/papers/files/adf0160-liuA.pdf)\n2. [Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization](http://proceedings.mlr.press/v28/dembczynski13.pdf)\n3. [Thresholding Classifiers to Maximize F1 Score](https://arxiv.org/pdf/1402.1892.pdf)\n","frontmatter":{"date":"2017-08-21","excerpt":"客户下次会购买什么","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8c8c8","images":{"fallback":{"src":"/static/44ecdc2c59cb9580edd37451edcc3e59/3ccfa/instacart.jpg","srcSet":"/static/44ecdc2c59cb9580edd37451edcc3e59/c55bf/instacart.jpg 284w,\n/static/44ecdc2c59cb9580edd37451edcc3e59/5a510/instacart.jpg 568w,\n/static/44ecdc2c59cb9580edd37451edcc3e59/3ccfa/instacart.jpg 1136w","sizes":"(min-width: 1136px) 1136px, 100vw"},"sources":[{"srcSet":"/static/44ecdc2c59cb9580edd37451edcc3e59/0ef98/instacart.webp 284w,\n/static/44ecdc2c59cb9580edd37451edcc3e59/d90d1/instacart.webp 568w,\n/static/44ecdc2c59cb9580edd37451edcc3e59/84dcc/instacart.webp 1136w","type":"image/webp","sizes":"(min-width: 1136px) 1136px, 100vw"}]},"width":1600,"height":1200}}},"title":"Kaggle Instacart复购预测竞赛回顾","slug":"/blog/kaggle-instacart-market-basket-analysis-retrospect/"}},{"rawMarkdownBody":"\n## 太长不看，说重点\n\n如果你有以下烦恼：\n1. 你希望做一个前后端分离的Python Web项目，后端只写REST接口，前端通过React或者Vue来开发\n2. 你希望部署的时候，前后端不分离。前端npm run build完的HTML/CSS/JavaScript静态文件，通过Python Web框架来Serve\n3. 部署时前后端不分离的一个重要原因，是因为你要做一个Python Package，通过setup.py来打包，最终打好的whl包可以发布到PyPI。\n   需要的人pip install一下就可以装上\n\n那么这篇博客可能会对你有帮助。\n\n## 业界趋势\n\n传统来说，Python世界的数据可视化方案可以分为两大流派。一派是科学计算阵营，以matplotlib为典型代表的服务端绘图方案：终端用户通常在Jupyter \nNotebook中，编写短短几行Python代码即可获得图片形式的绘图结果。易用性的优势显而易见，开发人员和用户，都只需要了解Python即可。但与此同时，\n由于输出是图片，就无法实现比如悬停提示，拖拽变更等用户交互功能。\n\n与之对应的，另一派则是Web开发阵营，HTML/CSS/JavaScript的客户端绘图方案。这种方案具有天然的可交互优势，但长期以来，缺点也很明显，\n主流的Python Web框架所提供的视图层解决方案是HTML模板（比如[Django Template](https://docs.djangoproject.com/zh-hans/3.2/topics/templates/), \nFlask生态下常用的[Jinja2](https://jinja.palletsprojects.com/en/3.0.x/)）。所谓模板，可以简单认为就是字符串拼接，\n通过Python代码来动态传入变量，生成静态的HTML + CSS + JavaScript）。在这个过程中，生成的JavaScript配合模板渲染好的HTML，提供一定程度的用户交互。\n即便对于一个熟练的Python Web开发来说，整个过程也不可谓不别扭，毕竟交互的核心逻辑其实还是JavaScript来实现的。\n\n与此同时，前端社区这些年来发展一直很迅猛，前后端分离已经是既成事实。如今更为现实的选择，是将整个前端解决方案，让渡给NodeJS/npm，享受丰富的前端生态。\n后端开发语言，无论是Python也好，Java亦或是Go，只提供REST接口就可以了。\n\n## 项目背景\n\n[sqllineage](https://github.com/reata/sqllineage) 是我开发的一个SQL血缘解析库。用户传入一个多语句的SQL文本，sqllineage可以解析出所有的输入表和输出表，\n并以有向无环图DAG的形式，进行可视化展示。\n\n![sqllineage](../images/sqllineage.png)\n\n在1.0版本的sqllineage中，血缘图可视化是通过matplotlib + graphviz来实现的。graphviz是一个C写的绘图库，但这里我没有用到其绘图功能，\n只是调用了一个dot函数，获得图的布局数据（简单说就是所有节点的坐标，每个点应该画在哪里），然后传回给matplotlib进行绘制。\n```python\ndef draw_lineage_graph(graph: DiGraph) -> None:\n    try:\n        import matplotlib.pyplot as plt\n        from matplotlib.colors import colorConverter\n        from matplotlib.patches import FancyArrowPatch\n        from matplotlib.path import Path\n    except ImportError as e:\n        raise ImportError(\"Matplotlib required for draw()\") from e\n    except RuntimeError:\n        logger.error(\"Matplotlib unable to open display\")\n        raise\n    try:\n        import pygraphviz  # noqa\n    except ImportError as e:\n        raise ImportError(\"requires pygraphviz\") from e\n    pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\", args=\"-Grankdir='LR'\")\n    edge_color, node_color, font_color = \"#9ab5c7\", \"#3499d9\", \"#35393e\"\n    arrowsize = font_size = radius = 10\n    node_size = 30\n    ha, va = \"left\", \"bottom\"\n    nx.draw(\n        graph,\n        pos=pos,\n        with_labels=True,\n        edge_color=edge_color,\n        arrowsize=arrowsize,\n        node_color=node_color,\n        node_size=node_size,\n        font_color=font_color,\n        font_size=font_size,\n        horizontalalignment=ha,\n        verticalalignment=va,\n    )\n    # selfloop edges\n    for edge in nx.selfloop_edges(graph):\n        x, y = pos[edge[0]]\n        arrow = FancyArrowPatch(\n            path=Path.circle((x, y + radius), radius),\n            arrowstyle=\"-|>\",\n            color=colorConverter.to_rgba_array(edge_color)[0],\n            mutation_scale=arrowsize,\n            linewidth=1.0,\n            zorder=1,\n        )\n        plt.gca().add_patch(arrow)\n    plt.show()\n```\n完整的代码参见[这里](https://github.com/reata/sqllineage/blob/v1.0.2/sqllineage/drawing.py) 。\n\n这个方案实现了我的初步需求，但也带来了一些制约，比如\n- graphviz是一个C库，在Windows上安装较为繁琐。经常有用户和我抱怨，pip install sqllineage已经成功了，但是绘图功能用不了。我看了之后，\n  通常是graphviz本身没装成功，或者pygraphviz，这个Python调用C的绑定库安装没成功。\n- 当节点非常多非常密集的时候，用户没有办法手动介入去拖动来调整图的布局。\n- 我后续想做一些更好的交互，比如用户鼠标悬停在某张表上，所有这张表的上游和下游表以及相关的边高亮。尤其当字段级别的血缘功能完成后，\n  会有更多定制化的可视化需求，这时候JS的图可视化生态的优势会更大。\n\n于是我决定，在1.1版本中，把它改成JavaScript来绘图，Web的解决方案一劳永逸，不用再担心操作系统的兼容问题。之前的 `draw_lineage_graph`\n这个函数弹出matplotlib的绘图，如今改为调用Flask，启动一个Web服务。这个Web Server的根路径，用GET方法调用时，返回index.html。\nindex.html，以及它关联的所有css和javascript文件，是通过前端npm run build打包出来的。用POST方法调用时，入参是SQL脚本的路径，\n则会返回JSON形式表示的DAG数据结构。\n```python\napp = Flask(\n    __name__,\n    static_url_path=\"\",\n    static_folder=os.path.join(os.path.dirname(__file__), STATIC_FOLDRE),\n)\nCORS(app)\n\n\n@app.route(\"/\")\ndef index():\n    return app.send_static_file(\"index.html\")\n\n\n@app.route(\"/lineage\", methods=[\"POST\"])\ndef lineage():\n    # this is to avoid circular import\n    from sqllineage.runner import LineageRunner\n\n    req_args = Namespace(**request.get_json())\n    sql = extract_sql_from_args(req_args)\n    resp = LineageRunner(sql).to_cytoscape()\n    return jsonify(resp)\n\n\ncli = sys.modules[\"flask.cli\"]\ncli.show_server_banner = lambda *x: None  # type: ignore\n\n\ndef draw_lineage_graph(**kwargs) -> None:\n    port = kwargs.pop(\"p\", DEFAULT_PORT)\n    querystring = urlencode({k: v for k, v in kwargs.items() if v})\n    print(f\" * SQLLineage Running on http://localhost:{port}/?{querystring}\")\n    app.run(port=port)\n```\n完整的代码参见[这里](https://github.com/reata/sqllineage/blob/v1.1.4/sqllineage/drawing.py)。\n\n前端工程单独放在[sqllineagejs](https://github.com/reata/sqllineage/tree/v1.1.4/sqllineagejs) 这个目录下，\n是一个标准的通过npm来开发和构建的前端项目，通过`npm run build`就可以打包出相应的HTML/CSS/JavaScript文件，内部调用的是create-react-app脚本，\n这里不多赘述。\n\n到这里，如果是本地开发，或者当做普通的application部署到服务器上，那么一切问题都搞定了。Flask的app.run()，就会启动相应的Server，\n部署时前后端代码不分离。即使对性能有要求的话，也依然支持将Flask应用通过gunicorn或者uwsgi来做部署，然后用专用的Web Server（比如Nginx）来Serve静态文件。\n\n但是，我需要发布的，是一个PyPI Package，这带来了额外的工作量。\n\n## 打包静态文件到Python Package\n\n目前Python社区的主流做法，是通过wheel这个库，调用python setup.py bdist_wheel命令，构建出一个whl包。这个包可以上传到PyPI，\n也可以离线分享并直接通过pip安装。我们需要搞定的问题，就是把静态文件，加入到这个whl包里。\n\n要达成这个目标，有各种各样的办法。但我有一个最主要的设计原则：**Python项目是通过setuptools构建的，JavaScript项目是通过npm构建的。我不希望再额外写一个shell脚本，包装两个构建系统**。\n考虑到我这个项目，主要是Python项目，我希望依然可以通过setuptools来构建。\n\n其次，我希望用户可以通过`pip install git+https://github.com/reata/sqllineage.git`直接从GitHub仓库进行源码安装。\n\n注意这两种安装方式有显著的不同：通过PyPI（的whl文件）安装或者直接离线安装whl文件，前端静态文件是处于已经构建好的状态，被打包在whl中，\n用户不需要在本地准备NodeJS/npm环境。而对于源码安装，前端代码是在用户本地环境构建的。想象一个带有C扩展的Python Package，\n源码安装时需要本地的编译环境，也是类似的情况。\n\n为了达成上面这些目标，需要进行下面这些配置：\n\n### package_data\n默认setup.py的构建只会包含package内所有的Python文件。如果你需要在最终构建的程序包中包含非Python文件，那么需要通过\npackage_data来指定，它的值是一个键值对，键是包名，值是包含数据文件的所有相对路径的列表。注意这里的相对路径是无法递归包含里面的子目录的。\n而典型的npm run build出来的目录结构如下：\n```\nbuild\n  - static\n    - css\n      *.chunk.css\n    - js\n      *.chunk.js\n  favicon.ico\n  index.html\n  manifest.json\n  robots.txt\n```\n所以我指定了两层相对路径为：\n```\npackage_data={\"\": [f\"build/*\", f\"build/**/**/*\"]}\n```\n\n可参考[官方文档](https://docs.python.org/3/distutils/setupscript.html#installing-package-data)\n\n### cmdclass\n\ncmdclass是setuptools提供的hook，通过覆盖相应的cmdclass，可以定制化地插入相应的代码，比如这里我们需要的调用npm构建前端代码。\n可参考[官方文档](https://docs.python.org/3/distutils/extending.html) 。由于我们既要保证whl类型的安装，也要保证源码tar.gz的安装。\n于是我们需要覆盖的hook，应该是bdist_wheel和sdist的交集：\n\n简单测试一下可知，二者都会调用egg_info这个hook：\n```shell\n$ python setup.py bdist_wheel | grep running\nrunning bdist_wheel\nrunning build\nrunning build_py\nrunning egg_info\nrunning install\nrunning install_lib\nrunning install_egg_info\nrunning install_scripts\n$ python setup.py sdist | grep running\nrunning sdist\nrunning egg_info\nrunning check\n```\n\n那么覆盖的代码如下：\n```python\nclass EggInfoWithJS(egg_info):\n    \"\"\"\n    egginfo is a hook both for\n        1) building source code distribution (python setup.py sdist)\n        2) building wheel distribution (python setup.py bdist_wheel)\n        3) installing from source code (python setup.py install) or pip install from GitHub\n    In this step, frontend code will be built to match MANIFEST.in list so that later the static files will be copied to\n    site-packages correctly as package_data. When building a distribution, no building process is needed at install time\n    \"\"\"\n\n    def run(self) -> None:\n        static_path = os.path.join(NAME, STATIC_FOLDRE)\n        if os.path.exists(static_path) or \"READTHEDOCS\" in os.environ:\n            pass\n        else:\n            js_path = \"sqllineagejs\"\n            use_shell = True if platform.system() == \"Windows\" else False\n            subprocess.check_call(\n                shlex.split(\"npm install\"), cwd=js_path, shell=use_shell\n            )\n            subprocess.check_call(\n                shlex.split(\"npm run build\"), cwd=js_path, shell=use_shell\n            )\n            shutil.move(os.path.join(js_path, STATIC_FOLDRE), static_path)\n        super().run()\n\n\nsetup(\n    ...\n    packages=find_packages(exclude=(\"tests\",)),\n    package_data={\"\": [f\"{STATIC_FOLDRE}/*\", f\"{STATIC_FOLDRE}/**/**/*\"]},\n    cmdclass={\"egg_info\": EggInfoWithJS},\n)\n```\n\n这里有几点值得注意的：\n- windows下不认环境变量，所以调用subprocess时，需要指定shell=True，否则找不到npm命令\n- 默认打包好的前端静态文件在sqllineagejs/build目录下，构建完成后，我们手动将其移动到sqllineage/build\n- 如果sqllineage/build文件夹存在，那么将会跳过前端构建过程。因为前端代码构建速度比较慢，本地没必要重复执行。而真正发布时，是在GitHub Action的CI机器上执行的，\n  每次都是重新拉取代码，不会有跳过构建的情况。（另一个例外情况是构建READTHEDOCS文档时，对应的环境没有npm，此时我们也不需要构建前端代码）\n\n参见最终完整版的[setup.py](https://github.com/reata/sqllineage/blob/v1.1.4/setup.py)。到这里，我们成功实现了，\n在构建Python Package时打包前端代码。是时候告别Python Web模板了，来享受现代化的前端解决方案吧。\n","frontmatter":{"date":"2021-06-05","excerpt":"为你的Python项目引入现代化的前端解决方案","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/446ecdd36bcfdc25b6ce7c574052397a/8a036/npm-pip.png","srcSet":"/static/446ecdd36bcfdc25b6ce7c574052397a/2a9e9/npm-pip.png 256w,\n/static/446ecdd36bcfdc25b6ce7c574052397a/c7bd0/npm-pip.png 512w,\n/static/446ecdd36bcfdc25b6ce7c574052397a/8a036/npm-pip.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/446ecdd36bcfdc25b6ce7c574052397a/36766/npm-pip.webp 256w,\n/static/446ecdd36bcfdc25b6ce7c574052397a/a79fe/npm-pip.webp 512w,\n/static/446ecdd36bcfdc25b6ce7c574052397a/2d898/npm-pip.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1600,"height":1200}}},"title":"在构建Python Package时打包前端代码","slug":"/blog/build-frontend-bundle-in-a-python-package/"}},{"rawMarkdownBody":"\n作为一个自学的程序员，工作快满7年，我终于把计算机专业本科生该学的课基本都学完了。\n\n感谢世界顶级学府开放的这么多网络公开课程。没有它们，就没有今天的我。作为回馈，也把走过的路整理整理，以飨有缘的读者。\n\n## 计算机科学导论\n\n[Harvard CS50: Introduction to Computer Science](https://pll.harvard.edu/course/cs50-introduction-computer-science)\n\n哈佛大学的计算机科学导论课程，各类概念都涉及到一些。趣味性十足，教授边撕书边讲二分搜索，我这辈子应该忘不了。\n\n这门课不是一定要上，毕竟导论课程介绍的概念，所希望培养的思维模式，在之后的课程里都会详细涉及。但如果你还没有决定，转行计算机是否适合你，\n那CS50是非常好的选择。\n\n## 编程入门\n\n[Fundamentals of Computing](https://online.rice.edu/courses/computer-fundamentals)\n\n莱斯大学的编程入门系列课程，一边介绍计算的基本概念，一边讲解Python编程基础。\n\n寓教于乐，每堂课的课后编程作业都是一个小游戏，从最简单的石头剪刀布，到类似雷电的卷轴射击，乃至策略型游戏无尽的饼干。\n作为从小到大的游戏玩家，这是入门编程再好不过的课程了。\n\n完成这门课后，给定明确的输入和输出，我能写一些简单的函数了。GitHub上各种项目的README，我突然发觉好些我都能看得懂，会使用了。\n这种感觉不异于读书时玩GTA:SA，突然有一天我发觉我能听懂路人说的英语时的喜悦感。\n\n这里Python是我个人的选择，但语言不是重点，重点是入门阶段至少要掌握一门编程语言。它是你和计算机沟通的桥梁。\n\n## 算法和数据结构\n\n[Princeton: Algorithm Part I](https://online.princeton.edu/node/201)\n\n[Princeton: Algorithm Part II](https://online.princeton.edu/node/166)\n\n对于任何一个只要了解一点点数据结构的人来说，红黑树都必定是一个如雷贯耳的名字。而讲授这门课的Sedgewick教授，正是红黑树的两位命名人之一。\n\n算法和数据结构是一把钥匙，入门的时候可能不知道这把钥匙能开什么门，但后来有不止一天，我都很庆幸，幸好我带了这把钥匙。\n我依然记得课程上讲述用并查集来为灌溉系统建模时，内心深处感受到的震撼，一切都充满了美感。而没有算法和数据结构，这一切都是不可能的。\n\n这门课是用Java讲的，我觉得作为程序世界的通用语言，Java是学习数据结构最合适的选择了。\nPython的str, list, dict, set抽象程度已经太高，损失了很多信息。而Java程序员天然就需要知道LinkedList和ArrayList有什么区别。\n\n当然Java也不是宇宙的尽头，未来想深入计算机底层的时候，再用C的视角来看数据结构也不迟。那时你会发现，内存不过是个大数组，\n寄存器、缓存是来自另一个世界的东西，而构建在内存这个大数组之上，不断抽象出的种种数据结构，是多么美妙。\n\n## 程序设计\n\n[Udacity: CS212 - Design of Computer Programs](https://www.udacity.com/course/design-of-computer-programs--cs212)\n\n入门到这个阶段，基本概念都有了，我也顺利找到了工作。当时做服务端开发，参考别人的代码，参考官方文档，依样画葫芦，我也能写出线上能用的代码了。\n\n但是当我开始研究我所用到的框架是怎么写出来的，当我试图封装一些库给其他人来用，我依然觉得无从下手。这个时候欠缺的是抽象能力。\n\n很多人推荐在这个阶段读[SICP](https://mitpress.mit.edu/sites/default/files/sicp/index.html)，我个人读过之后的感觉，书里的概念固然很好，\n但是因为它是用Scheme写的，也没有什么很好的IDE，做起练习来实在是一种痛苦。而Udacity上的这门课由Google研究主管Peter Norvig讲授，\n使用Python语言，对我就十分友好。\n\n上这门课，至今仍然受用的一句话是说，当你想要重构或者开发新功能时，“概念上的变动，应该等比于代码量的变动”，否则这里可能有坏代码的味道。\n\n说起来，SICP里也有一句至理名言，“越是变化，事情越是它原本的样子”，代码和人生，都是这样。\n\n## 计算机系统原理\n\n[CMU: 15-213 - Intro to Computer Systems](https://www.cs.cmu.edu/~213/)\n\n我知道计算机只认识0和1，我也知道怎么让我写的Python/Java程序运行起来。可是它们之间有多遥远的距离？我不知道。\n\nJava的一次编写到处执行意味着什么，谁在替你负重前行？C写的程序为什么针对不同平台要编译不同的二进制文件？我也不知道。\n\n买电脑时看CPU总说的L1/L2缓存，到底是什么？深度学习为什么基于GPU来计算要快得多？我还是不知道。\n\n太多太多不知道，而正如这门课的开宗明义讲到：大多数计算机的课程都是在教你构建抽象，以获得封装后带来的更强大的能力，为此你不需要操心很多的细节。\n而这门课的不同在于，我们要看看抽象的背后是什么。\n\n我专门学了C语言编程，很努力想把Lab都做下来，但没能成功。饶是如此，上完这门课，我还是有一种奇经八脉都被打通的感觉。\n让我们时刻牢记，\"Abstraction is good, but don't forget reality\".\n\n这门课的参考书，[CSAPP](https://csapp.cs.cmu.edu/)，经典中的经典。\n\n## 编译原理\n\n[Stanford CS143: Compilers](https://web.stanford.edu/class/cs143/)\n\n读过CSAPP，懂得了汇编的基本概念，就已经有了学习编译原理的先决条件。\n\n我从没想过我会学习《编译原理》，我对汇编和硬件没有特别高的兴趣。我也听不止一个科班出身的朋友和我讲过，《编译原理》是他们大学里唯一一门一个字都没听懂的课程。\n\n学习这门课的动机，来自于一个非常直接的需求，我在做SQL血缘解析，而开源的SQL Parser已经渐渐无法满足我的需要，生成的语法树在很多边界条件下总是和我想的不一样。\n当我想要为其做一些定制化，由于缺乏相应的理论储备，我再度发觉无从下手。\n\n这样目的也就非常明确了，关于编译器后端的部分，代码生成、优化，我不求甚解。对于编译器前端，词法分析、语法分析、语义分析，我尽力了解形式语言的理论，\n以及各种基础实现。这样当我回过头去看像Antlr这样的parser生成工具，就没有什么障碍了。\n\n话说回来，形式语言的分类谱系[乔姆斯基体系](https://zh.wikipedia.org/wiki/%E4%B9%94%E5%A7%86%E6%96%AF%E5%9F%BA%E8%B0%B1%E7%B3%BB)\n命名于语言学家乔姆斯基，这是我大学上英语专业课《语言学》时认得的名字。那时老师提过，MIT的语言学系是用计算机和数理统计的理念来研究语言学的。\n我当时想，总有一天我要看看他们是怎么玩的。\n\n念念不忘，必有回响。\n\n## 数据库系统原理\n\n[CMU: 15-445 - Database Systems](https://15445.courses.cs.cmu.edu/)\n\n作为大数据工程师，执行计划、查询优化、事务处理、故障恢复，这些得懂吧。否则只是做一个SQL boy未免太没有意思了。\n\n学完再去看Hive, Spark, HBase, Presto, Clickhouse, 会多一种微妙的眼光。倒不是说否定大数据的创新性，但颠覆式创新？谈不上吧。更像是螺旋上升。\n\n一切都是从数据库中来，到数据仓库/大数据系统里去。太阳底下无新事。\n\n这里多提一嘴Andy Pavlo教授，绝对是我上过所有公开课里，最有个性的。这门课竟然有自己的DJ，有片头和片尾。哪怕你对数据库不感兴趣，上这门课你都不会后悔的。\n\n## 机器学习\n\n[Stanford CS229: Machine Learning](https://cs229.stanford.edu/)\n\n我当年是读了《大数据时代》这本书，决定转行做程序员的。那个时候，书里介绍的更多的神奇应用，其实是机器学习的范畴。我本身有社科的背景，数理统计学得还不错。\n所以很长时间内，我是想去搞机器学习的。\n\n但是工作后就发现，理想很丰满，很多公司的条件其实根本不具备。信息化、数字化做得怎么样？大数据系统有吗？不会走路，很难跑起来呀。与其做一个听上去高大上，\n实则每天在写SQL的分析师/算法工程师，我还是觉得能写代码更幸福一些。\n\n话说远了，即使没能用上，即使如今深度学习火了，这门讲授传统机器学习的课程还是很值得。最重要的是了解机器学习的思路。\n一般意义上的编程是程序员了解问题后，告诉计算机我要做什么以及我要怎么做。而机器学习的思路是告诉计算机我要求解的问题，以及给它看这些是以前别人解过的题，\n让计算机自动学习出其中的模式。\n\n有了这种思路，在全民AI的今天，谁还不能当个调包侠呢？但时至今日，我还能不能手推逻辑回归的公式，就是另一个问题了。\n\n## 密码学\n\n[Stanford: Cryptography I](https://crypto.stanford.edu/~dabo/courses/OnlineCrypto/)\n\n其实这门课我在入门阶段，大概学算法与数据结构前后就上了。原因无它，就想扩展一下对于比特币和区块链的兴趣，看看它们背后的理论源头是啥。\n\n从功利的角度讲，学这门课对于找工作没太大帮助，除非你一心想投身币圈，而我显然不是。\n\n但从学东西的角度，《密码学》又满足了我很多的好奇心:\n\n比如我这才知道，原来没有绝对意义上安全的加密，只有当前阶段最厉害的计算机花上几十年还暴力穷举不出结果的加密。 \n\n比如我以前一直想不通，信息要加密传输，那密钥怎么传，总不能明文吧，还是说只能线下给？想不明白。但等学了非对称加密，看过RSA算法，真的感慨，\n发明这些算法的人拥有全世界最性感的大脑。\n\n另外，《密码学》的另一大馈赠：我从此也成为了八卦Alice和Bob罗曼史的一份子。\n\n## TODO: Stay Hungry, Stay Foolish\n\n- 算法设计和分析：Stanford CS106B/X\n- 操作系统：MIT 6.S081/UCB CS162\n- 计算机网络：Stanford CS144\n- 分布式系统：CMU 15-418/Stanford CS149/MIT 6.824\n- 计算机图形学：UCSB Games101\n- 自然语言处理：Stanford CS224n\n- 图像识别：Stanford CS231n\n- 知识图谱：Stanford CS520\n","frontmatter":{"date":"2022-02-15","excerpt":"学习这件事，还是得靠自己","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#48a8b8","images":{"fallback":{"src":"/static/308add5c8b481a9e524195de10da532b/1f157/MOOC.png","srcSet":"/static/308add5c8b481a9e524195de10da532b/378dc/MOOC.png 144w,\n/static/308add5c8b481a9e524195de10da532b/b4934/MOOC.png 288w,\n/static/308add5c8b481a9e524195de10da532b/1f157/MOOC.png 575w","sizes":"(min-width: 575px) 575px, 100vw"},"sources":[{"srcSet":"/static/308add5c8b481a9e524195de10da532b/86dff/MOOC.webp 144w,\n/static/308add5c8b481a9e524195de10da532b/2db4f/MOOC.webp 288w,\n/static/308add5c8b481a9e524195de10da532b/2cf74/MOOC.webp 575w","type":"image/webp","sizes":"(min-width: 575px) 575px, 100vw"}]},"width":1600,"height":909.9130434782609}}},"title":"计算机科学自学网络公开课","slug":"/blog/computer-science-open-online-course/"}},{"rawMarkdownBody":"\n[SQLLineage](https://github.com/reata/sqllineage)是一个基于Python的SQL血缘分析工具。2020年9月v1.0版本发布的时候，\n写过一篇介绍的[博客](https://reata.github.io/blog/sqllineage-a-sql-lineage-analysis-tool/)。\nv1.0版本发布时，SQLLineage基本上达到甚至稍稍超越了我19年开始做这个项目时候的设想。此前在做离线任务调度系统的我，所希望的就是有一个库，\n我把SQL告诉它，它可以回答我这段SQL读了什么表，最终又写入了哪张表。这样我可以灵活地去做SQL任务的依赖检查和推荐。\n\nv1.0版本针各类SQL语句做了充分的测试，就表级血缘这一功能来讲，是生产环境高度可用的。而我甚至还做了基于DAG的血缘可视化。\n\n之后的两个版本：\n\n- v1.1版本的迭代，将血缘可视化，由matplotlib+graphviz的服务端绘图方案，切换为基于Cytoscape.js的客户端绘图方案。交互性更强的同时，也摆脱了对graphviz的依赖。\n  Windows下安装C Library及其Python binding并不那么容易，之前经常有用户抱怨pygraphviz安装不上。移除之后，兼容性的问题也就同步解决了。\n\n- v1.2版本，随着React+MaterialUI的引入，SQLLineage成为了一个独立的Web App，我在Heroku上部署了一个[DEMO](https://sqllineage.herokuapp.com/)站点，这样即使没有Python背景的人，\n  也可以使用SQLLineage了，进一步增强了易用性。（关于在Python Package里发布前端代码，感兴趣可以看[这篇博客](https://reata.github.io/blog/build-frontend-bundle-in-a-python-package/)）\n\n就我个人的想法，做到这个阶段，是到了事了拂衣去的时候了。\n\n**然而社区里一直有做字段级别血缘的呼声，长期以来，[Column-Level Lineage #103](https://github.com/reata/sqllineage/issues/103)\n都是讨论最热烈、回应最多的Issue。**\n\n“屈服”之前，我也想讲讲我的“苦衷”。\n\n## 为什么不做\n\n最直观的原因：做不准。\n\nSQLLineage从设计上是一个静态SQL代码分析工具，只分析SQL文本，不会尝试去执行SQL。不执行，也不会查询元数据。我希望采用通用的SQL parser，\n来支持尽可能多的SQL方言，这同时这也意味着如果要去对接每种SQL的元数据系统，作为一个个人项目，哪怕是社区驱动的开源项目，工作量都是无法想象的。\n\n对于表级别血缘来说，静态分析几乎已经足够了。我们抽取出的表名，也还是本文形式返回。如果出现一张表在SQL语句中用到，但其实不存在的情况，作为工具，\n我就原样返回，并不承诺它在元数据中存在。如果SQL里用到表时没有写库名的前缀，不知道来自哪个库。这也没关系，没有库名就是默认的库，\n用户自然知道默认的库是哪个，我并不需要、也不可能比用户知道更多。\n\n但对于字段级别血缘来说，类似的方式就不太友好。当多表关联的时候，一个字段没有写表名的前缀，它可能来自任何一张关联表。或者SELECT *的情况，\n不查元数据，根本不可能知道到底SELECT了哪些字段。这种情形下，表级血缘可以绘制成精美的有向无环图，字段级血缘强行绘制的话，恐怕大部分点都是割裂的。\n这对于用户来说，显然不够。\n\n当时的想法是，也许有一天，会有一个类似Hive metastore这样的元数据系统，作为统一的标准出现，各大数据库/数据仓库厂商都支持这一标准。\n或者哪怕没有数据库层面的标准，当Python社区出现了类似JDBC这样的标准接口（相比JDBC，Python社区的DB-API2显然做得还不够），那么我再来考虑字段血缘不迟。\n\n然而社区在通过自己的形式告诉我，我们等不了。那我想，来都来了，就试着做做吧。就算最后不行，起码我可以说出《飞越疯人院》里的名言，**\"At least I tried\"**.\n\n## 设计原则\n\n任何事情都不是空中楼阁，好也罢、坏也罢，之前的基础很大程度上决定之后的选择，所谓“路径依赖”。我考虑了一下，下面两个原则，\n是至少在第一个版本的字段血缘中，我会选择继续坚持的：\n\n1. **SQLLineage主要仍将会是一个静态代码分析工具。** 这也意味着在做字段血缘时，我们需要容忍信息不准确。也许在未来的版本中， \n   我们可以提供一个插件的机制，由用户来注册元数据信息。通过这些元数据信息，我们来完善光凭静态分析得到的不准确的血缘。\n   但无论如何，我不会只依赖元数据，我想不要一个没有元数据就跑不了的工具。\n2. **字段血缘的DAG，不能独立于表级血缘。** 理想情况下，只维护一份统一的血缘图。至于实现，可以有两种：1）把DAG做到字段粒度，通过一些转换，\n   可以计算出表级血缘的DAG。用关系型数据库的概念来做类比，就像先做一张明细表，在明细表的基础上聚合可以得到汇总表。2）通过属性图的形式来建模，\n   可以参照JanusGraph的[文档](https://docs.janusgraph.org/schema/)。表和字段分别是两种类型的节点，同时另外还有两种类型的边，\n   其一是字段到表的所属关系，其二是字段与字段、表与表的血缘关系。\n\n## 需要解答的问题\n\n有了上面两点设计原则，我们就可以再来思考一下代码实现层面的细节了。有这么一些问题是要优先想明白的：\n\n1. **用什么样的数据结构来表示字段血缘？** 之前表级别是用`networkx`中的`DiGraph`来存储DAG的，其中表是节点，而表级血缘是边。从表级别来说，\n   非常直观。那么字段呢？\n\n**一种可行的答案：** 结合我以前做图数据库应用时的一些经验，汇总级别的操作，也就是针对图的OLAP，其实并不是那么好做。图数据库一般用Gremlin语言，\n做OLAP都有些别扭，`networkx`的API我就更加不确定能否支持了。所以我个人更偏向属性图的建模形式。\n\n2. **如何处理`SELECT *`**。比如下面这个SQL，我不知道tab2里有哪些字段。\n\n```sql\nINSERT OVERWRITE tab1\nSELECT * FROM tab2;\n```\n\n**一种可行的答案：** 增加一个`Column *`的虚拟节点，作为特殊的字段。这样这里的字段血缘可以表示为`tab2.* -> tab1.*`\n\n3. **如何处理多表关联时，字段名没有写表/别名前缀的情况**。比如下面这个SQL，我不知道col2是来自tab2还是tab3。\n\n```sql\nINSERT OVERWRITE tab1\nSELECT col2\nFROM tab2\nJOIN tab3\nON tab2.col1 = tab3.col1\n```\n\n**一种可行的答案：** 维护两条血缘，`tab2.col2 -> tab1.col2和tab3.col2 -> tab1.col2`。同时，在这两条边上做特殊的标记，\n表明需要元数据才能进一步剪裁。而前端可视化时，也可以将这种血缘用特殊的方式标记出来，比如绘制成虚线，悬浮时才显示。\n\n## 功能实现的拆解\n\n大方向上悬而未决的问题定下来了，接下来是具体的功能拆解：\n\n1. 字段逻辑的原子性操作识别：字段别名，CASE WHEN，函数，表达式等。\n2. 子查询的识别，以及子查询的血缘结果如何提升到语句级别的血缘。\n3. 多表关联时，字段到表的归属确定。\n4. 将语句级别的血缘信息，组装为SQL文件级别（文件级别可以包含多条SQL语句）。\n\n拆解到这个程度之后，设计层面的事情就都搞定了。之后就是纯堆人力的问题，以及修复一些边边角角的bug，花时间总能搞定。\n\n## 而今迈步从头越\n\n所有这些，回过头看起来很显而易见的抉择，当时前后总计思考了大约3个月吧。最后花了一整个国庆假期，前后10天完成了编码。\n\n字段血缘做下来，感悟很多：\n\n- 字段血缘是一个中等难度的项目，写出来还是很有成就感的。之前1.2版本，只包含表级血缘，大约400行Python代码。到1.3版本，暴增到约800行。\n  核心的抽象，增加了很多。之前比如Union语句，比如子查询，比如CTE，都不怎么需要特别关心，做字段血缘时就需要非常小心去处理。\n\n- 字段血缘需要更深入地了解SQL解析的过程。我用的`sqlparse`是不校验语法的，为了支持各种SQL方言，它只是尽力去解析，不保证结果一定合法。\n  所有有时候我需要微调一下parser，以便生成符合设想、更好操作的AST。比如窗口函数就是我自己定制的，原生的`sqlparse`会把窗口函数解析成两个独立的部分。\n  没有编译原理的基础知识，做这个会稍稍有些吃力。相比较做表级血缘的时候，其实AST对我来说就是个普通的数据结构，我会用就行了，不关心它是怎么生成的。\n\n- 字段血缘相比表级血缘，更需要可视化。表级别的，直接给出输入表和输出表，信息损失也不大。但如果字段血缘还是只有命令行界面的输出，\n  那就非常难以使用了。庆幸之前做了JavaScript的可视化方案改造，Cytoscape.js在换到字段血缘后几乎只是稍微改了一下配置，\n  就可以直接展示后端给出的新的数据结构了。这里是比我最初设想节省了时间的。\n\n## 展望未来\n\nSQLLineage还会有v1.4吗，我想肯定是会有的，毕竟还有源源不断的issue。之前没有社区的呼声和支持，字段血缘是不可能做出来的。\n之后我也会持续地花时间去迭代，响应社区的需求。\n\n现阶段主要考虑到的大的功能点包括：\n\n1. 为了更好做好字段血缘，元数据插件必不可少。目前的设想是，定义一个元数据类，一些接口，比如list_tables，list_columns。由用户个人来实现。\n   这样SQLLineage只要拿到用户提供的元数据类，就可以针对不准确的信息进行校准。系统可以维护一个最常用的Hive metastore的元数据类实现，\n   也作为其它实现的参考。\n\n2. 目前的很多Issue集中在对更多方言特殊语法的支持，Snowflake的Merge语句，比如SQLServer的Assignment Operator。要做更好的支持，\n   当前的`sqlparse`已经有点力不从心，我在它基础上做了非常多的monkey_patch，这个项目很可惜也已经不再活跃开发了。\n   暂时的一个想法是用Antlr来生成自己的parser，未来我会先用Java版本论证一下可行性。\n\n3. SQLLineage现在更多是作为Web应用，受到关注。这和我最开始主要考虑命令行应用+Python库的定位，其实有一定的背离。\n   在之前的版本中，我也有在刻意隐藏一些Python的API。因为字段血缘不做，API变动就会很大。现阶段，我认为SQLLineage作为Python库的潜力是没有完全释放的。\n   字段血缘相对稳定之后，Python API也是可以重点考虑的一个方向。\n\n总之感谢社区的支持。开源是一件非常有意思、也有情怀的事情。Let's have some more.\n","frontmatter":{"date":"2022-02-20","excerpt":"字段级别的SQL血缘解析","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/362f4339aa3414bb99e76b0225170f32/0b631/column_lineage.jpg","srcSet":"/static/362f4339aa3414bb99e76b0225170f32/8e5b4/column_lineage.jpg 239w,\n/static/362f4339aa3414bb99e76b0225170f32/e452a/column_lineage.jpg 478w,\n/static/362f4339aa3414bb99e76b0225170f32/0b631/column_lineage.jpg 955w","sizes":"(min-width: 955px) 955px, 100vw"},"sources":[{"srcSet":"/static/362f4339aa3414bb99e76b0225170f32/f6788/column_lineage.webp 239w,\n/static/362f4339aa3414bb99e76b0225170f32/f9fbb/column_lineage.webp 478w,\n/static/362f4339aa3414bb99e76b0225170f32/8684b/column_lineage.webp 955w","type":"image/webp","sizes":"(min-width: 955px) 955px, 100vw"}]},"width":1600,"height":1584.9214659685863}}},"title":"SQLLineage v1.3：迈向字段血缘","slug":"/blog/sqllineage-towards-column-lineage/"}}]}},"pageContext":{}},"staticQueryHashes":[]}